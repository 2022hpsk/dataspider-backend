{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86562e278d1a1f5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T06:15:56.023244Z",
     "start_time": "2025-01-12T06:15:55.737377Z"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'datacrawl_backend (Python 3.8.20)' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import random\n",
    "import json\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ff8976e3c8742ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T06:16:27.005702Z",
     "start_time": "2025-01-12T06:16:20.911827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标题: (1) Home / X\n"
     ]
    }
   ],
   "source": [
    "#先运行命令再运行此代码块 & Start-Process \"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\" -- \" --remote-debugging-port=9530 --user-data-dir=\"\"D:\\twitter_data\"\"\"\n",
    "chromedriver_path = r\"C:\\Users\\admin\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe\"\n",
    "\n",
    "option = Options()\n",
    "option.add_experimental_option(\"debuggerAddress\", \"127.0.0.1:9530\")\n",
    "\n",
    "# 创建 ChromeDriver 服务\n",
    "service = Service(chromedriver_path)\n",
    "\n",
    "# 初始化 WebDriver\n",
    "driver = webdriver.Chrome(service=service, options=option)\n",
    "\n",
    "# 去除 WebDriver 痕迹\n",
    "driver.execute_cdp_cmd(\"Page.addScriptToEvaluateOnNewDocument\", {\n",
    "    \"source\": \"\"\"\n",
    "    Object.defineProperty(navigator, 'webdriver', {\n",
    "      get: () => undefined\n",
    "    });\n",
    "    \"\"\"\n",
    "})\n",
    "\n",
    "# 设置隐式等待\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# 测试访问页面\n",
    "driver.get(\"https://www.x.com\")\n",
    "print(\"标题:\", driver.title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc2aa75e2792d9b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T06:16:46.613018Z",
     "start_time": "2025-01-12T06:16:46.597546Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver import ActionChains\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_posts_byday(keyword, start_date, end_date, is_live): \n",
    "    posts_lists = []\n",
    "    post_index = set() #帖子独立标识集合去重\n",
    "    # if is_live:\n",
    "    #     driver.get(f\"https://x.com/search?q={keyword}%20lang%3Aen%20until%3A{end_date}%20since%3A{start_date}&src=typed_query&f=live\") # 查询条件 src=typed_query：表示这是一个手动输入的查询。f=live：表示只检索实时搜索结果\n",
    "    # else:\n",
    "    #     driver.get(f\"https://x.com/search?q={keyword}&src=typed_query&f=top\") # 查询条件 src=typed_query：表示这是一个手动输入的查询。\n",
    "    driver.get(f\"https://x.com/Ruslan558775\")\n",
    "    time.sleep(5)  # 等待页面加载完成\n",
    "\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    scroll_step = 4500  # 每次向下滚动的像素数\n",
    "    scroll_count = 0\n",
    "    previous_post_count = 0  # 用于记录上一次的帖子ID数量\n",
    "    max_attempts = 5  # 当数据条数不变的最大允许次数\n",
    "    attempts = 0\n",
    "    garbages = []\n",
    "    isgarbage = 0\n",
    "\n",
    "    # 保存路径\n",
    "    base_dir = r\"C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\"\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    if is_live:\n",
    "        posts_file = os.path.join(base_dir, f\"[latest rank]posts_{keyword}_{start_date}_{end_date}.json\")\n",
    "    else:\n",
    "        posts_file = os.path.join(base_dir, f\"[top rank]posts_{keyword}_{start_date}_{end_date}.json\")\n",
    "\n",
    "        while True:\n",
    "            page_source = driver.page_source\n",
    "            soup = BeautifulSoup(page_source, 'lxml')\n",
    "            post_tags = soup.select('div[data-testid=\"cellInnerDiv\"]')\n",
    "            # post_tags = soup.select('article[data-testid=\"tweet\"]')\n",
    "            for post_tag in post_tags:\n",
    "                transform_value = post_tag['style'].split('translateY(')[1].split('px)')[0]\n",
    "                # print(transform_value)\n",
    "                if float(transform_value) > 450 and transform_value not in post_index: # 不想漏掉第一个帖子，所以宁可错采一些\n",
    "                    post_index.add(transform_value)\n",
    "\n",
    "                    # 提取用户名\n",
    "                    username_tag = post_tag.select_one('div[data-testid=\"User-Name\"]')\n",
    "                    if username_tag:\n",
    "                        username = username_tag.get_text(strip=True)\n",
    "                        # 检查用户名是否包含垃圾词\n",
    "                        if any(garbage in username for garbage in garbages):\n",
    "                            isgarbage = 1\n",
    "                            continue  # 跳过当前帖子，不再提取ID\n",
    "                        username = username_tag.select_one('span').text.strip() if username_tag.select_one('span') else ''\n",
    "                    isgarbage = 0\n",
    "\n",
    "                    post_content = \"\"\n",
    "                    tweet_text_div = post_tag.select_one('div[data-testid=\"tweetText\"]')\n",
    "                    if tweet_text_div:\n",
    "                        # 遍历所有子元素，按顺序提取文字、图片和链接\n",
    "                        for element in tweet_text_div.children:\n",
    "                            if element.name == 'span':  # 处理文本\n",
    "                                post_content += element.text.strip() + \" \"\n",
    "                            elif element.name == 'img':  # 处理图片\n",
    "                                alt_text = element.get('alt', '')\n",
    "                                if alt_text:\n",
    "                                    post_content += f\"{alt_text} \"\n",
    "                            elif element.name == 'a':  # 处理链接\n",
    "                                href = element.get('href', '')\n",
    "                                if href:\n",
    "                                    post_content += f\"{href} \"\n",
    "                                    post_content += element.text.strip() + \" \"\n",
    "\n",
    "                    # 输出最终提取的内容\n",
    "                    # print(post_content)\n",
    "\n",
    "                    # print(post_content)\n",
    "\n",
    "                    # 查找包含aria-label的div标签，提取转评赞以及帖子id\n",
    "                    if start_date < '2023-01-01':\n",
    "                        aria_label = post_tag.find('div', attrs={'aria-label': re.compile(r'.*likes.*')}) # likes views\n",
    "                    else:\n",
    "                        aria_label = post_tag.find('div', attrs={'aria-label': re.compile(r'.*views.*')}) # likes views\n",
    "                    if aria_label:\n",
    "                        # 从aria-label中提取数据\n",
    "                        label_text = aria_label['aria-label']\n",
    "\n",
    "                        # 初始化数据\n",
    "                        replies = reposts = likes = bookmarks = views = 0\n",
    "\n",
    "                        if 'repl' in label_text:\n",
    "                            match = re.search(r'(\\d+)\\s+repl', label_text)\n",
    "                            replies = int(match.group(1)) if match else 0\n",
    "\n",
    "                        if 'repost' in label_text:\n",
    "                            match = re.search(r'(\\d+)\\s+repost', label_text)\n",
    "                            reposts = int(match.group(1)) if match else 0\n",
    "\n",
    "                        if 'like' in label_text:\n",
    "                            match = re.search(r'(\\d+)\\s+like', label_text)\n",
    "                            likes = int(match.group(1)) if match else 0\n",
    "\n",
    "                        if 'bookmark' in label_text:\n",
    "                            match = re.search(r'(\\d+)\\s+bookmark', label_text)\n",
    "                            bookmarks = int(match.group(1)) if match else 0\n",
    "                        if 'view' in label_text:\n",
    "                            # views = int(re.search(r'(\\d+)\\s+view', label_text).group(1))\n",
    "                            # 检查是否匹配到了 \"view\" 和数字\n",
    "                            match = re.search(r'(\\d+)\\s+view', label_text)\n",
    "                            if match:\n",
    "                                views = int(match.group(1))\n",
    "                            else:\n",
    "                                views = 0  # 如果没有匹配到，设置默认值为0\n",
    "\n",
    "                        # 查找帖子ID和用户ID\n",
    "                        post_id_tag = aria_label.find_all('div', class_='css-175oi2r r-18u37iz r-1h0z5md r-13awgt0')\n",
    "                        if post_id_tag:\n",
    "                            # 找到第四个div，提取其中的链接\n",
    "                            post_id_link = post_id_tag[3].find('a', href=True)\n",
    "                            if post_id_link:\n",
    "                                post_id = post_id_link['href'].split('/')[3]  # 从URL中提取帖子ID\n",
    "                                user_id = post_id_link['href'].split('/')[1]\n",
    "                            else:\n",
    "                                post_id = ''\n",
    "                                user_id = ''\n",
    "                        else:\n",
    "                            post_id = ''\n",
    "                            user_id = ''\n",
    "                    else:\n",
    "                        replies = reposts = likes = bookmarks = views = 0\n",
    "                        post_id = ''\n",
    "                        user_id = ''\n",
    "                        break\n",
    "\n",
    "                    # 提取帖子发布时间\n",
    "                    time_tag = post_tag.find('time', attrs={'datetime': True})\n",
    "                    if time_tag and 'datetime' in time_tag.attrs:\n",
    "                        datetime_str = time_tag['datetime']\n",
    "                        # 解析ISO格式的日期时间字符串\n",
    "                        dt = datetime.strptime(datetime_str, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "                        # 格式化为“年-月-日 时:分:秒”格式\n",
    "                        post_time = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    else:\n",
    "                        post_time = ''\n",
    "\n",
    "                    # 提取hashtags，确保唯一性，使用集合去重\n",
    "                    hashtags = set()\n",
    "                    hashtag_tags = post_tag.select('a[href*=\"/hashtag/\"]')\n",
    "                    for tag in hashtag_tags:\n",
    "                        hashtag = tag.get_text(strip=True)\n",
    "                        hashtags.add(hashtag)\n",
    "\n",
    "                    # 提取图片和视频的链接\n",
    "                    media_urls = set()  # 使用集合来自动去重\n",
    "                    img_tags = post_tag.find_all('img')\n",
    "                    for img in img_tags:\n",
    "                        img_url = img.get('src')\n",
    "                        if img_url and 'profile_images' not in img_url and 'emoji' not in img_url:\n",
    "                            media_urls.add(img_url)\n",
    "                    media_urls = list(media_urls)\n",
    "\n",
    "                    # 提取@了哪些用户\n",
    "                    # at_usernames = set()  # 使用集合确保唯一性\n",
    "                    # at_usernames.update(re.findall(r'@(\\w+)', post_content))  # 查找所有 @ 后的用户名\n",
    "\n",
    "                    # 提取帖子中的所有链接（使用正则表达式从文本中匹配URL）\n",
    "                    urls = set()  # 使用集合去重\n",
    "\n",
    "                    # 匹配以 http 或 https 开头的 URL\n",
    "                    url_pattern = re.compile(r'https?://[^\\s]+')\n",
    "\n",
    "                    # 在帖子文本中查找所有符合条件的URL\n",
    "                    urls.update(url_pattern.findall(post_content))\n",
    "\n",
    "                    # 查找回复的多个用户\n",
    "                    reply_users = []\n",
    "                    reply_tag = post_tag.find('div', class_='css-146c3p1 r-bcqeeo r-1ttztb7 r-qvutc0 r-37j5jr r-a023e6 r-rjixqe r-16dba41') # 查找包含回复信息的 div\n",
    "                    if reply_tag:\n",
    "                        # 获取所有回复用户的 a 标签\n",
    "                        reply_user_tags = reply_tag.find_all('a')\n",
    "                        for reply_user_tag in reply_user_tags:\n",
    "                            # 提取用户名并添加到列表\n",
    "                            reply_users.append(reply_user_tag.text.strip())\n",
    "                    # 构建帖子数据，新增字段\n",
    "                    web_object = {\n",
    "                        \"postUrl\": f\"https://x.com/{user_id}/status/{post_id}\",\n",
    "                        \"text\": post_content,\n",
    "                        \"mid\": post_id,\n",
    "                        \"userName\": username,\n",
    "                        \"userId\": user_id,\n",
    "                        \"date\": post_time,\n",
    "                        \"likeNum\": likes,\n",
    "                        \"commentNum\": replies,\n",
    "                        \"repostNum\": reposts,\n",
    "                        \"viewNum\": views,\n",
    "                        \"bookmarks\": bookmarks,\n",
    "                        \"hashtags\": list(hashtags),\n",
    "                        \"mediaUrls\": media_urls,\n",
    "                        # \"atUsernames\": list(at_usernames),  # 新增字段：记录@的用户\n",
    "                        # \"urls\": list(urls),  # 新增字段：记录帖中的所有链接\n",
    "                        # \"replyUsers\": reply_users  # 新增字段：记录多个回复的用户\n",
    "                    }\n",
    "\n",
    "                    #  # 构建帖子数据\n",
    "                    # web_object = {\n",
    "                    #     \"text\": post_content,\n",
    "                    #     \"mid\": post_id,\n",
    "                    #     \"userName\": username,\n",
    "                    #     \"userId\": user_id,\n",
    "                    #     \"date\": post_time,\n",
    "                    #     \"likeNum\": likes,\n",
    "                    #     \"commentNum\": replies,\n",
    "                    #     \"repostNum\": reposts,\n",
    "                    #     \"viewNum\": views,\n",
    "                    #     \"bookmarks\": bookmarks\n",
    "                    # }\n",
    "\n",
    "                    posts_lists.append(web_object)\n",
    "\n",
    "                    # 保存 posts\n",
    "                    if os.path.exists(posts_file):\n",
    "                        # File exists, so read the existing data\n",
    "                        with open(posts_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                            existing_data = json.load(f)\n",
    "                        \n",
    "                        # Append the new posts data to the existing data\n",
    "                        existing_data.extend(posts_lists)\n",
    "\n",
    "                        # Write the updated data back to the file\n",
    "                        with open(posts_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "                        \n",
    "                        print(f\"Data appended to {posts_file}\")\n",
    "\n",
    "                    else:\n",
    "                        # File doesn't exist, create it and write the new data\n",
    "                        with open(posts_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                            json.dump(posts_lists, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "                        print(f\"File {posts_file} created with initial data.\")\n",
    "                    '''\n",
    "                    [目前]存储在posts.json文件中的每个帖子对象包含以下信息：\n",
    "\n",
    "                        text: 帖子的正文内容。\n",
    "                        mid: 帖子的ID。\n",
    "                        userName: 用户名。\n",
    "                        userId: 用户ID。\n",
    "                        date: 帖子的发布时间。\n",
    "                        likeNum: 点赞数。\n",
    "                        commentNum: 评论数。\n",
    "                        repostNum: 转发数。\n",
    "                        viewNum: 浏览数。\n",
    "                        bookmarks: 收藏数。\n",
    "                    ''' \n",
    "                \n",
    "            current_post_count = len(posts_lists)\n",
    "            print(f\"已收集到 {current_post_count} 个帖子\")\n",
    "\n",
    "            # if current_post_count >= 200:  # 如果爬取到的数据条数超过200，则停止爬取\n",
    "            #     break\n",
    "\n",
    "            if current_post_count == previous_post_count and not isgarbage: #防止遇到批量垃圾帖子，导致误识别为滑到底了\n",
    "                time.sleep(random.uniform(1, 2))\n",
    "                attempts += 1\n",
    "            else:\n",
    "                attempts = 0\n",
    "                previous_post_count = current_post_count\n",
    "\n",
    "            if attempts == max_attempts:\n",
    "                # 检测到页面错误信息，尝试点击'Retry'按钮...\n",
    "                if len(post_tags) > 0 and post_tags[-1].select_one('span').text.strip() == \"Something went wrong. Try reloading.\" if post_tags[-1].select_one('span') else '':\n",
    "                    print(f\"检测到'Retry'按钮，当前时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')},滚动了{scroll_count}次\")\n",
    "                    # 等待人工确认点击\n",
    "                    input(\"按Enter键以继续爬取...\")\n",
    "                    while True:  # 无限循环，直到成功点击\n",
    "                        try:\n",
    "                            # 等待6分钟（360秒）\n",
    "                            print(\"等待1分钟...\")\n",
    "                            time.sleep(60)  # 等待6分钟\n",
    "\n",
    "                            # 使用 CSS Selector 或 XPath 查找'Retry'按钮并确保它可点击\n",
    "                            retrybutton = WebDriverWait(driver, 3).until(\n",
    "                                EC.element_to_be_clickable((By.XPATH, \"//button[.//*[contains(text(),'Retry')]]\"))\n",
    "                            )\n",
    "\n",
    "                            # 滚动到按钮所在位置\n",
    "                            driver.execute_script(\"arguments[0].scrollIntoView();\", retrybutton)\n",
    "\n",
    "                            # 强制点击按钮\n",
    "                            driver.execute_script(\"arguments[0].click();\", retrybutton)\n",
    "                            print(f\"点击'Retry'按钮成功，等待5秒后继续爬取，当前时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "                            time.sleep(15)  # 等待15秒后继续爬取\n",
    "                            attempts = 0\n",
    "                            previous_post_count = current_post_count\n",
    "                            break  # 成功点击后跳出循环\n",
    "\n",
    "                        except Exception as e:\n",
    "                            # print(f\"未能找到'Retry'按钮或点击失败，错误信息: {e}\")\n",
    "                            print(\"没找到按钮\")\n",
    "\n",
    "                print(\"多次滚动后没有新数据，等30秒\")\n",
    "                time.sleep(30)\n",
    "\n",
    "            if attempts > max_attempts:  # 如果连续6次没有新数据，停止爬取\n",
    "                print(f\"多次滚动后没有新数据，已尝试 {attempts}\")\n",
    "                # break\n",
    "                user_input = input(\"请输入0停止爬取，1继续爬取: \")  # 获取用户输入\n",
    "                if user_input == \"0\":\n",
    "                    print(\"停止爬取。\")\n",
    "                    break  # 停止爬取\n",
    "                elif user_input == \"1\":\n",
    "                    print(\"继续爬取...\")\n",
    "                    attempts = 0  # 重置尝试次数，继续爬取\n",
    "                else:\n",
    "                    print(\"无效输入，请输入0或1。\")\n",
    "\n",
    "            driver.execute_script(f\"window.scrollBy(0, {scroll_step});\")\n",
    "            scroll_count += 1\n",
    "            time.sleep(random.uniform(2, 3))  # 等待加载更多内容\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b263ab8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json created with initial data.\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "已收集到 4 个帖子\n",
      "已收集到 4 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "已收集到 19 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "已收集到 28 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "已收集到 35 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "已收集到 45 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "已收集到 51 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "已收集到 54 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "已收集到 62 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "已收集到 70 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "已收集到 75 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "已收集到 84 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "已收集到 91 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "已收集到 99 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "已收集到 107 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "已收集到 113 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "已收集到 123 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "已收集到 131 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "已收集到 138 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "已收集到 144 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "已收集到 150 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "已收集到 162 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "已收集到 170 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "已收集到 177 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_momo1_2024-01-01_2025-03-21.json\n",
      "已收集到 184 个帖子\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mget_posts_byday\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmomo1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2024-01-01\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2025-03-21\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 326\u001b[0m, in \u001b[0;36mget_posts_byday\u001b[1;34m(keyword, start_date, end_date, is_live)\u001b[0m\n\u001b[0;32m    324\u001b[0m driver\u001b[38;5;241m.\u001b[39mexecute_script(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow.scrollBy(0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscroll_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m);\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    325\u001b[0m scroll_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 326\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "get_posts_byday('momo1', '2024-01-01', '2025-03-21', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b8ec5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_posts_day_by_day(keyword, start_date, end_date, live_flag, search_by_year=False):\n",
    "    # 转换日期字符串为 datetime 对象\n",
    "    start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    delta = 15\n",
    "    if search_by_year:\n",
    "        delta = 365\n",
    "\n",
    "    # 循环遍历每一天\n",
    "    current_date = start_date\n",
    "    if live_flag:\n",
    "        print(\"正在爬取latest搜索结果...\")\n",
    "        while current_date < end_date:\n",
    "            # 格式化日期为字符串（你可以根据需要调整格式）\n",
    "            start_date_str = current_date.strftime(\"%Y-%m-%d\")\n",
    "            end_date_str = (current_date + timedelta(days=delta)).strftime(\"%Y-%m-%d\")\n",
    "            print(f\"爬取日期：{start_date_str}~{end_date_str}\")\n",
    "\n",
    "            # 调用爬虫函数获取当天的数据\n",
    "            get_posts_byday(keyword, start_date_str, end_date_str, live_flag)\n",
    "\n",
    "            # 增加一天\n",
    "            current_date += timedelta(days=delta)\n",
    "    else:\n",
    "        print(\"正在爬取top搜索结果...\")\n",
    "        while current_date < end_date:\n",
    "            # 格式化日期为字符串（你可以根据需要调整格式）\n",
    "            start_date_str = current_date.strftime(\"%Y-%m-%d\")\n",
    "            end_date_str = (current_date + timedelta(days=delta)).strftime(\"%Y-%m-%d\")\n",
    "            print(f\"爬取日期：{start_date_str}~{end_date_str}\")\n",
    "\n",
    "            # 调用爬虫函数获取当天的数据\n",
    "            get_posts_byday(keyword, start_date_str, end_date_str, live_flag)\n",
    "\n",
    "            # 增加一天\n",
    "            current_date += timedelta(days=delta)\n",
    "\n",
    "# 调用函数\n",
    "# 采集历史记录\n",
    "# get_posts_day_by_day('trump garbage', '2024-10-25', '2024-11-07', False)\n",
    "# get_posts_day_by_day('biden garbage', '2024-10-25', '2024-11-07', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf66ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在爬取top搜索结果...\n",
      "爬取日期：2020-09-01~2020-09-16\n",
      "已收集到 0 个帖子\n",
      "File C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json created with initial data.\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "已收集到 5 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "已收集到 9 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "已收集到 13 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "已收集到 18 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "已收集到 21 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "已收集到 25 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "已收集到 26 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "已收集到 32 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "已收集到 35 个帖子\n",
      "已收集到 35 个帖子\n",
      "已收集到 35 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "已收集到 43 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "已收集到 44 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "已收集到 47 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "已收集到 49 个帖子\n",
      "已收集到 49 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "已收集到 50 个帖子\n",
      "已收集到 50 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "已收集到 55 个帖子\n",
      "已收集到 55 个帖子\n",
      "已收集到 55 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-01_2020-09-16.json\n",
      "已收集到 60 个帖子\n",
      "已收集到 60 个帖子\n",
      "已收集到 60 个帖子\n",
      "已收集到 60 个帖子\n",
      "已收集到 60 个帖子\n",
      "已收集到 60 个帖子\n",
      "多次滚动后没有新数据，等30秒\n",
      "已收集到 60 个帖子\n",
      "多次滚动后没有新数据，已尝试 6\n",
      "继续爬取...\n",
      "已收集到 60 个帖子\n",
      "已收集到 60 个帖子\n",
      "已收集到 60 个帖子\n",
      "已收集到 60 个帖子\n",
      "已收集到 60 个帖子\n",
      "多次滚动后没有新数据，等30秒\n",
      "已收集到 60 个帖子\n",
      "多次滚动后没有新数据，已尝试 6\n",
      "停止爬取。\n",
      "爬取日期：2020-09-16~2020-10-01\n",
      "已收集到 0 个帖子\n",
      "File C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json created with initial data.\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "已收集到 4 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "已收集到 10 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "已收集到 11 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "已收集到 16 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "已收集到 20 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "已收集到 23 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "已收集到 26 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "已收集到 27 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "已收集到 28 个帖子\n",
      "已收集到 28 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "已收集到 35 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "已收集到 38 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "已收集到 39 个帖子\n",
      "已收集到 39 个帖子\n",
      "已收集到 39 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "已收集到 41 个帖子\n",
      "已收集到 41 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "已收集到 42 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "已收集到 46 个帖子\n",
      "已收集到 46 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-09-16_2020-10-01.json\n",
      "已收集到 49 个帖子\n",
      "已收集到 49 个帖子\n",
      "已收集到 49 个帖子\n",
      "已收集到 49 个帖子\n",
      "已收集到 49 个帖子\n",
      "已收集到 49 个帖子\n",
      "多次滚动后没有新数据，等30秒\n",
      "已收集到 49 个帖子\n",
      "多次滚动后没有新数据，已尝试 6\n",
      "停止爬取。\n",
      "爬取日期：2020-10-01~2020-10-16\n",
      "已收集到 0 个帖子\n",
      "File C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json created with initial data.\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "已收集到 5 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "已收集到 8 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "已收集到 10 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "已收集到 13 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "已收集到 15 个帖子\n",
      "已收集到 15 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "已收集到 16 个帖子\n",
      "已收集到 16 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "已收集到 17 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "已收集到 22 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "已收集到 25 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "已收集到 26 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "已收集到 30 个帖子\n",
      "已收集到 30 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "已收集到 33 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "已收集到 34 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "已收集到 38 个帖子\n",
      "已收集到 38 个帖子\n",
      "已收集到 38 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "已收集到 39 个帖子\n",
      "已收集到 39 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-01_2020-10-16.json\n",
      "已收集到 41 个帖子\n",
      "已收集到 41 个帖子\n",
      "已收集到 41 个帖子\n",
      "已收集到 41 个帖子\n",
      "已收集到 41 个帖子\n",
      "已收集到 41 个帖子\n",
      "多次滚动后没有新数据，等30秒\n",
      "已收集到 41 个帖子\n",
      "多次滚动后没有新数据，已尝试 6\n",
      "停止爬取。\n",
      "爬取日期：2020-10-16~2020-10-31\n",
      "已收集到 0 个帖子\n",
      "File C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json created with initial data.\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "已收集到 5 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "已收集到 6 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "已收集到 11 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "已收集到 14 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "已收集到 16 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "已收集到 22 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "已收集到 24 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "已收集到 25 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "已收集到 31 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "已收集到 33 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "已收集到 41 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "已收集到 46 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "已收集到 48 个帖子\n",
      "已收集到 48 个帖子\n",
      "已收集到 48 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "已收集到 49 个帖子\n",
      "已收集到 49 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "已收集到 51 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "已收集到 52 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "已收集到 54 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-16_2020-10-31.json\n",
      "已收集到 55 个帖子\n",
      "已收集到 55 个帖子\n",
      "已收集到 55 个帖子\n",
      "已收集到 55 个帖子\n",
      "已收集到 55 个帖子\n",
      "已收集到 55 个帖子\n",
      "多次滚动后没有新数据，等30秒\n",
      "已收集到 55 个帖子\n",
      "多次滚动后没有新数据，已尝试 6\n",
      "停止爬取。\n",
      "爬取日期：2020-10-31~2020-11-15\n",
      "已收集到 0 个帖子\n",
      "File C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json created with initial data.\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "已收集到 2 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "已收集到 8 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "已收集到 11 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "已收集到 14 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "已收集到 16 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "已收集到 21 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "已收集到 25 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "已收集到 33 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "已收集到 37 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "已收集到 45 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "已收集到 49 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "已收集到 55 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "已收集到 56 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "已收集到 57 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "已收集到 58 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "已收集到 59 个帖子\n",
      "已收集到 59 个帖子\n",
      "已收集到 59 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "已收集到 61 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-10-31_2020-11-15.json\n",
      "已收集到 64 个帖子\n",
      "已收集到 64 个帖子\n",
      "已收集到 64 个帖子\n",
      "已收集到 64 个帖子\n",
      "已收集到 64 个帖子\n",
      "已收集到 64 个帖子\n",
      "多次滚动后没有新数据，等30秒\n",
      "已收集到 64 个帖子\n",
      "多次滚动后没有新数据，已尝试 6\n",
      "停止爬取。\n",
      "爬取日期：2020-11-15~2020-11-30\n",
      "已收集到 0 个帖子\n",
      "File C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json created with initial data.\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "已收集到 5 个帖子\n",
      "已收集到 5 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "已收集到 10 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "已收集到 12 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "已收集到 13 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "已收集到 17 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "已收集到 21 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "已收集到 22 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "已收集到 26 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "已收集到 30 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "已收集到 38 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "已收集到 51 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "已收集到 56 个帖子\n",
      "已收集到 56 个帖子\n",
      "已收集到 56 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "已收集到 59 个帖子\n",
      "已收集到 59 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "已收集到 60 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "已收集到 61 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-15_2020-11-30.json\n",
      "已收集到 62 个帖子\n",
      "已收集到 62 个帖子\n",
      "已收集到 62 个帖子\n",
      "已收集到 62 个帖子\n",
      "已收集到 62 个帖子\n",
      "已收集到 62 个帖子\n",
      "多次滚动后没有新数据，等30秒\n",
      "已收集到 62 个帖子\n",
      "多次滚动后没有新数据，已尝试 6\n",
      "停止爬取。\n",
      "爬取日期：2020-11-30~2020-12-15\n",
      "已收集到 0 个帖子\n",
      "File C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json created with initial data.\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "已收集到 5 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "已收集到 8 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "已收集到 10 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "已收集到 16 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "已收集到 21 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "已收集到 26 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "已收集到 31 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "已收集到 32 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "已收集到 35 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "已收集到 43 个帖子\n",
      "已收集到 43 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "已收集到 47 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "已收集到 54 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "已收集到 55 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "已收集到 56 个帖子\n",
      "已收集到 56 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "已收集到 59 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-11-30_2020-12-15.json\n",
      "已收集到 63 个帖子\n",
      "已收集到 63 个帖子\n",
      "已收集到 63 个帖子\n",
      "已收集到 63 个帖子\n",
      "已收集到 63 个帖子\n",
      "已收集到 63 个帖子\n",
      "多次滚动后没有新数据，等30秒\n",
      "已收集到 63 个帖子\n",
      "多次滚动后没有新数据，已尝试 6\n",
      "停止爬取。\n",
      "爬取日期：2020-12-15~2020-12-30\n",
      "已收集到 0 个帖子\n",
      "File C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json created with initial data.\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "已收集到 5 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "已收集到 9 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "已收集到 10 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "已收集到 12 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "已收集到 17 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "已收集到 22 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "已收集到 27 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "已收集到 32 个帖子\n",
      "已收集到 32 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "已收集到 40 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "已收集到 43 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "已收集到 45 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "已收集到 48 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "已收集到 50 个帖子\n",
      "已收集到 50 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "已收集到 51 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "已收集到 55 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "已收集到 57 个帖子\n",
      "已收集到 57 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "已收集到 58 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "已收集到 59 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-15_2020-12-30.json\n",
      "已收集到 61 个帖子\n",
      "已收集到 61 个帖子\n",
      "已收集到 61 个帖子\n",
      "已收集到 61 个帖子\n",
      "已收集到 61 个帖子\n",
      "已收集到 61 个帖子\n",
      "多次滚动后没有新数据，等30秒\n",
      "已收集到 61 个帖子\n",
      "多次滚动后没有新数据，已尝试 6\n",
      "停止爬取。\n",
      "爬取日期：2020-12-30~2021-01-14\n",
      "已收集到 0 个帖子\n",
      "File C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json created with initial data.\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "已收集到 3 个帖子\n",
      "已收集到 3 个帖子\n",
      "已收集到 3 个帖子\n",
      "已收集到 3 个帖子\n",
      "已收集到 3 个帖子\n",
      "已收集到 3 个帖子\n",
      "多次滚动后没有新数据，等30秒\n",
      "已收集到 3 个帖子\n",
      "多次滚动后没有新数据，已尝试 6\n",
      "继续爬取...\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "已收集到 8 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "已收集到 14 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "已收集到 16 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "已收集到 18 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "已收集到 25 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "已收集到 26 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "已收集到 32 个帖子\n",
      "已收集到 32 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "已收集到 34 个帖子\n",
      "已收集到 34 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "已收集到 40 个帖子\n",
      "已收集到 40 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "已收集到 41 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "已收集到 43 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "已收集到 46 个帖子\n",
      "已收集到 46 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "已收集到 47 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "已收集到 48 个帖子\n",
      "已收集到 48 个帖子\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "Data appended to C:\\Users\\admin\\codespace\\twittercrawler\\twitter\\json\\[top rank]posts_pepe the frog_2020-12-30_2021-01-14.json\n",
      "已收集到 53 个帖子\n",
      "已收集到 53 个帖子\n",
      "已收集到 53 个帖子\n",
      "已收集到 53 个帖子\n",
      "已收集到 53 个帖子\n",
      "已收集到 53 个帖子\n",
      "多次滚动后没有新数据，等30秒\n",
      "已收集到 53 个帖子\n",
      "多次滚动后没有新数据，已尝试 6\n"
     ]
    }
   ],
   "source": [
    "get_posts_day_by_day('pepe the frog', '2020-09-01', '2020-12-31', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb5e9da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_posts_day_by_day('biden', '2018-10-25', '2019-11-07', False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9939a863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在爬取top搜索结果...\n",
      "爬取日期：2014-01-01~2015-01-01\n",
      "0\n",
      "0\n",
      "48\n",
      "158\n",
      "210\n",
      "219\n",
      "803\n",
      "1194\n",
      "1618\n",
      "2138\n",
      "2761\n",
      "3187\n",
      "3830\n",
      "4244\n",
      "4363\n",
      "4482\n",
      "4906\n",
      "5549\n",
      "已收集到 12 个帖子\n",
      "219\n",
      "803\n",
      "1194\n",
      "1618\n",
      "2138\n",
      "2761\n",
      "3187\n",
      "3830\n",
      "4244\n",
      "4363\n",
      "4482\n",
      "4906\n",
      "5549\n",
      "5942\n",
      "6499\n",
      "7142\n",
      "7654\n",
      "8078\n",
      "8721\n",
      "9364\n",
      "9483\n",
      "9604\n",
      "10081\n",
      "已收集到 22 个帖子\n",
      "4482\n",
      "4906\n",
      "5549\n",
      "5942\n",
      "6499\n",
      "7142\n",
      "7654\n",
      "8078\n",
      "8721\n",
      "9364\n",
      "9483\n",
      "9604\n",
      "10081\n",
      "10593\n",
      "10984\n",
      "11519\n",
      "11999\n",
      "12118\n",
      "12787\n",
      "13267\n",
      "13386\n",
      "13505\n",
      "13624\n",
      "13743\n",
      "13862\n",
      "14382\n",
      "14501\n",
      "已收集到 36 个帖子\n",
      "8721\n",
      "9364\n",
      "9483\n",
      "9604\n",
      "10081\n",
      "10593\n",
      "10984\n",
      "11519\n",
      "11999\n",
      "12118\n",
      "12787\n",
      "13267\n",
      "13386\n",
      "13505\n",
      "13624\n",
      "13743\n",
      "13862\n",
      "14382\n",
      "14501\n",
      "15030\n",
      "15550\n",
      "15671\n",
      "16095\n",
      "16214\n",
      "16333\n",
      "16812\n",
      "16931\n",
      "17050\n",
      "17169\n",
      "17288\n",
      "17781\n",
      "18159\n",
      "18278\n",
      "18397\n",
      "18516\n",
      "18635\n",
      "18734\n",
      "19246\n",
      "已收集到 55 个帖子\n",
      "13743\n",
      "13862\n",
      "14382\n",
      "14501\n",
      "15030\n",
      "15550\n",
      "15671\n",
      "16095\n",
      "16214\n",
      "16333\n",
      "16812\n",
      "16931\n",
      "17050\n",
      "17169\n",
      "17288\n",
      "17781\n",
      "18159\n",
      "18278\n",
      "18397\n",
      "18516\n",
      "18635\n",
      "18734\n",
      "19246\n",
      "19758\n",
      "19897\n",
      "20418\n",
      "20537\n",
      "20656\n",
      "20815\n",
      "20934\n",
      "21053\n",
      "21457\n",
      "21576\n",
      "21735\n",
      "22159\n",
      "22623\n",
      "22742\n",
      "22861\n",
      "22980\n",
      "23404\n",
      "已收集到 72 个帖子\n",
      "18278\n",
      "18397\n",
      "18516\n",
      "18635\n",
      "18734\n",
      "19246\n",
      "19758\n",
      "19897\n",
      "20418\n",
      "20537\n",
      "20656\n",
      "20815\n",
      "20934\n",
      "21053\n",
      "21457\n",
      "21576\n",
      "21735\n",
      "22159\n",
      "22623\n",
      "22742\n",
      "22861\n",
      "22980\n",
      "23404\n",
      "23901\n",
      "24020\n",
      "24505\n",
      "24624\n",
      "24743\n",
      "24842\n",
      "25363\n",
      "25482\n",
      "25994\n",
      "26398\n",
      "26517\n",
      "26636\n",
      "26875\n",
      "27315\n",
      "27434\n",
      "27553\n",
      "27672\n",
      "27791\n",
      "28215\n",
      "已收集到 91 个帖子\n",
      "22742\n",
      "22861\n",
      "22980\n",
      "23404\n",
      "23901\n",
      "24020\n",
      "24505\n",
      "24624\n",
      "24743\n",
      "24842\n",
      "25363\n",
      "25482\n",
      "25994\n",
      "26398\n",
      "26517\n",
      "26636\n",
      "26875\n",
      "27315\n",
      "27434\n",
      "27553\n",
      "27672\n",
      "27791\n",
      "28215\n",
      "28878\n",
      "29037\n",
      "29156\n",
      "已收集到 93 个帖子\n",
      "26875\n",
      "27315\n",
      "27434\n",
      "27553\n",
      "27672\n",
      "27791\n",
      "28215\n",
      "28878\n",
      "29037\n",
      "29156\n",
      "29647\n",
      "已收集到 93 个帖子\n",
      "31735\n",
      "31854\n",
      "31973\n",
      "32132\n",
      "32271\n",
      "已收集到 97 个帖子\n",
      "36109\n",
      "已收集到 97 个帖子\n",
      "40025\n",
      "40144\n",
      "已收集到 98 个帖子\n",
      "40025\n",
      "40144\n",
      "40263\n",
      "已收集到 98 个帖子\n",
      "40025\n",
      "40144\n",
      "40263\n",
      "40402\n",
      "已收集到 98 个帖子\n",
      "40025\n",
      "40144\n",
      "40263\n",
      "40402\n",
      "40541\n",
      "已收集到 98 个帖子\n",
      "40025\n",
      "40144\n",
      "40263\n",
      "40402\n",
      "40541\n",
      "40660\n",
      "已收集到 98 个帖子\n",
      "40025\n",
      "40144\n",
      "40263\n",
      "40402\n",
      "40541\n",
      "40660\n",
      "40799\n",
      "已收集到 98 个帖子\n",
      "多次滚动后没有新数据，等60秒\n",
      "40025\n",
      "40144\n",
      "40263\n",
      "40402\n",
      "40541\n",
      "40660\n",
      "40799\n",
      "40918\n",
      "已收集到 98 个帖子\n",
      "多次滚动后没有新数据，已尝试 6\n",
      "停止爬取。\n",
      "爬取日期：2015-01-01~2016-01-01\n",
      "0\n",
      "0\n",
      "48\n",
      "158\n",
      "210\n",
      "219\n",
      "610\n",
      "1130\n",
      "1552\n",
      "1977\n",
      "2400\n",
      "2825\n",
      "3494\n",
      "3916\n",
      "4435\n",
      "4924\n",
      "5295\n",
      "5717\n",
      "已收集到 12 个帖子\n",
      "219\n",
      "610\n",
      "1130\n",
      "1552\n",
      "1977\n",
      "2400\n",
      "2825\n",
      "3494\n",
      "3916\n",
      "4435\n",
      "4924\n",
      "5295\n",
      "5717\n",
      "5896\n",
      "6373\n",
      "6890\n",
      "7407\n",
      "8027\n",
      "8449\n",
      "8873\n",
      "9277\n",
      "9699\n",
      "10208\n",
      "10327\n",
      "已收集到 23 个帖子\n",
      "4435\n",
      "4924\n",
      "5295\n",
      "5717\n",
      "5896\n",
      "6373\n",
      "6890\n",
      "7407\n",
      "8027\n",
      "8449\n",
      "8873\n",
      "9277\n",
      "9699\n",
      "10208\n",
      "10327\n",
      "10446\n",
      "10859\n",
      "11243\n",
      "11783\n",
      "12176\n",
      "12675\n",
      "13099\n",
      "13707\n",
      "14131\n",
      "14250\n",
      "14674\n",
      "已收集到 34 个帖子\n",
      "9277\n",
      "9699\n",
      "10208\n",
      "10327\n",
      "10446\n",
      "10859\n",
      "11243\n",
      "11783\n",
      "12176\n",
      "12675\n",
      "13099\n",
      "13707\n",
      "14131\n",
      "14250\n",
      "14674\n",
      "15172\n",
      "15596\n",
      "16020\n",
      "16119\n",
      "16543\n",
      "16662\n",
      "17139\n",
      "17258\n",
      "17377\n",
      "17797\n",
      "18274\n",
      "18393\n",
      "18532\n",
      "18954\n",
      "19073\n",
      "已收集到 49 个帖子\n",
      "13707\n",
      "14131\n",
      "14250\n",
      "14674\n",
      "15172\n",
      "15596\n",
      "16020\n",
      "16119\n",
      "16543\n",
      "16662\n",
      "17139\n",
      "17258\n",
      "17377\n",
      "17797\n",
      "18274\n",
      "18393\n",
      "18532\n",
      "18954\n",
      "19073\n",
      "19497\n",
      "19995\n",
      "20455\n",
      "20875\n",
      "21350\n",
      "21825\n",
      "22302\n",
      "22762\n",
      "22881\n",
      "23356\n",
      "已收集到 58 个帖子\n",
      "18274\n",
      "18393\n",
      "18532\n",
      "18954\n",
      "19073\n",
      "19497\n",
      "19995\n",
      "20455\n",
      "20875\n",
      "21350\n",
      "21825\n",
      "22302\n",
      "22762\n",
      "22881\n",
      "23356\n",
      "23475\n",
      "23991\n",
      "24415\n",
      "24534\n",
      "24958\n",
      "25077\n",
      "25196\n",
      "25603\n",
      "25722\n",
      "25841\n",
      "26316\n",
      "26435\n",
      "26554\n",
      "26713\n",
      "26872\n",
      "已收集到 72 个帖子\n",
      "22762\n",
      "22881\n",
      "23356\n",
      "23475\n",
      "23991\n",
      "24415\n",
      "24534\n",
      "24958\n",
      "25077\n",
      "25196\n",
      "25603\n",
      "25722\n",
      "25841\n",
      "26316\n",
      "26435\n",
      "26554\n",
      "26713\n",
      "26872\n",
      "27515\n",
      "27634\n",
      "27753\n",
      "28177\n",
      "28319\n",
      "已收集到 76 个帖子\n",
      "26872\n",
      "27515\n",
      "27634\n",
      "27753\n",
      "28177\n",
      "28319\n",
      "28743\n",
      "28902\n",
      "29386\n",
      "29648\n",
      "已收集到 79 个帖子\n",
      "31766\n",
      "31885\n",
      "32004\n",
      "32123\n",
      "32242\n",
      "已收集到 83 个帖子\n",
      "35683\n",
      "已收集到 83 个帖子\n",
      "40371\n",
      "40762\n",
      "41411\n",
      "已收集到 85 个帖子\n",
      "44692\n",
      "45188\n",
      "已收集到 86 个帖子\n",
      "45446\n",
      "已收集到 86 个帖子\n",
      "45446\n",
      "45837\n",
      "已收集到 86 个帖子\n",
      "45446\n",
      "45837\n",
      "45956\n",
      "已收集到 86 个帖子\n",
      "45446\n",
      "45837\n",
      "45956\n",
      "46075\n",
      "已收集到 86 个帖子\n",
      "45446\n",
      "45837\n",
      "45956\n",
      "46075\n",
      "46194\n",
      "已收集到 86 个帖子\n",
      "多次滚动后没有新数据，等60秒\n",
      "45446\n",
      "45837\n",
      "45956\n",
      "46075\n",
      "46194\n",
      "46313\n",
      "已收集到 86 个帖子\n",
      "多次滚动后没有新数据，已尝试 6\n",
      "停止爬取。\n",
      "爬取日期：2016-01-01~2016-12-31\n",
      "0\n",
      "0\n",
      "48\n",
      "158\n",
      "210\n",
      "219\n",
      "338\n",
      "781\n",
      "1204\n",
      "1620\n",
      "2043\n",
      "2466\n",
      "2875\n",
      "3298\n",
      "3722\n",
      "4184\n",
      "4605\n",
      "5028\n",
      "5549\n",
      "已收集到 12 个帖子\n",
      "219\n",
      "338\n",
      "781\n",
      "1204\n",
      "1620\n",
      "2043\n",
      "2466\n",
      "2875\n",
      "3298\n",
      "3722\n",
      "4184\n",
      "4605\n",
      "5028\n",
      "5549\n",
      "5972\n",
      "6395\n",
      "6864\n",
      "7288\n",
      "7550\n",
      "7973\n",
      "8397\n",
      "8536\n",
      "9013\n",
      "9490\n",
      "9968\n",
      "10372\n",
      "已收集到 24 个帖子\n",
      "4605\n",
      "5028\n",
      "5549\n",
      "5972\n",
      "6395\n",
      "6864\n",
      "7288\n",
      "7550\n",
      "7973\n",
      "8397\n",
      "8536\n",
      "9013\n",
      "9490\n",
      "9968\n",
      "10372\n",
      "10728\n",
      "11207\n",
      "11611\n",
      "12090\n",
      "12439\n",
      "12748\n",
      "13139\n",
      "13258\n",
      "13629\n",
      "14278\n",
      "14417\n",
      "14841\n",
      "已收集到 36 个帖子\n",
      "9013\n",
      "9490\n",
      "9968\n",
      "10372\n",
      "10728\n",
      "11207\n",
      "11611\n",
      "12090\n",
      "12439\n",
      "12748\n",
      "13139\n",
      "13258\n",
      "13629\n",
      "14278\n",
      "14417\n",
      "14841\n",
      "15319\n",
      "15797\n",
      "16217\n",
      "16336\n",
      "16740\n",
      "16859\n",
      "17338\n",
      "17477\n",
      "17868\n",
      "18027\n",
      "18451\n",
      "18570\n",
      "18689\n",
      "19113\n",
      "19232\n",
      "已收集到 51 个帖子\n",
      "13629\n",
      "14278\n",
      "14417\n",
      "14841\n",
      "15319\n",
      "15797\n",
      "16217\n",
      "16336\n",
      "16740\n",
      "16859\n",
      "17338\n",
      "17477\n",
      "17868\n",
      "18027\n",
      "18451\n",
      "18570\n",
      "18689\n",
      "19113\n",
      "19232\n",
      "19656\n",
      "20299\n",
      "20723\n",
      "20985\n",
      "21376\n",
      "21495\n",
      "21919\n",
      "22343\n",
      "22734\n",
      "23158\n",
      "23389\n",
      "23508\n",
      "23627\n",
      "23746\n",
      "已收集到 65 个帖子\n",
      "18027\n",
      "18451\n",
      "18570\n",
      "18689\n",
      "19113\n",
      "19232\n",
      "19656\n",
      "20299\n",
      "20723\n",
      "20985\n",
      "21376\n",
      "21495\n",
      "21919\n",
      "22343\n",
      "22734\n",
      "23158\n",
      "23389\n",
      "23508\n",
      "23627\n",
      "23746\n",
      "24008\n",
      "24399\n",
      "24823\n",
      "已收集到 67 个帖子\n",
      "22734\n",
      "23158\n",
      "23389\n",
      "23508\n",
      "23627\n",
      "23746\n",
      "24008\n",
      "24399\n",
      "24823\n",
      "25343\n",
      "25747\n",
      "25866\n",
      "26260\n",
      "26379\n",
      "26498\n",
      "26617\n",
      "26736\n",
      "27160\n",
      "27527\n",
      "27646\n",
      "27765\n",
      "27884\n",
      "28547\n",
      "28809\n",
      "28928\n",
      "29127\n",
      "已收集到 83 个帖子\n",
      "27160\n",
      "27527\n",
      "27646\n",
      "27765\n",
      "27884\n",
      "28547\n",
      "28809\n",
      "28928\n",
      "29127\n",
      "29517\n",
      "29779\n",
      "已收集到 84 个帖子\n",
      "31687\n",
      "32111\n",
      "已收集到 85 个帖子\n",
      "36021\n",
      "已收集到 85 个帖子\n",
      "40355\n",
      "已收集到 85 个帖子\n",
      "45283\n",
      "45402\n",
      "已收集到 86 个帖子\n",
      "45561\n",
      "已收集到 86 个帖子\n",
      "47322\n",
      "已收集到 86 个帖子\n",
      "47322\n",
      "47441\n",
      "已收集到 86 个帖子\n",
      "47322\n",
      "47441\n",
      "47560\n",
      "已收集到 86 个帖子\n",
      "47322\n",
      "47441\n",
      "47560\n",
      "47679\n",
      "已收集到 86 个帖子\n",
      "多次滚动后没有新数据，等60秒\n",
      "47322\n",
      "47441\n",
      "47560\n",
      "47679\n",
      "47818\n",
      "已收集到 86 个帖子\n",
      "多次滚动后没有新数据，已尝试 6\n",
      "停止爬取。\n",
      "爬取日期：2016-12-31~2017-12-31\n",
      "0\n",
      "0\n",
      "48\n",
      "158\n",
      "210\n",
      "219\n",
      "623\n",
      "1272\n",
      "1696\n",
      "2120\n",
      "2511\n",
      "2936\n",
      "3361\n",
      "3837\n",
      "4344\n",
      "4821\n",
      "5470\n",
      "已收集到 11 个帖子\n",
      "219\n",
      "623\n",
      "1272\n",
      "1696\n",
      "2120\n",
      "2511\n",
      "2936\n",
      "3361\n",
      "3837\n",
      "4344\n",
      "4821\n",
      "5470\n",
      "5894\n",
      "6156\n",
      "6275\n",
      "6732\n",
      "7156\n",
      "7633\n",
      "8130\n",
      "8554\n",
      "8978\n",
      "9647\n",
      "9997\n",
      "10116\n",
      "已收集到 23 个帖子\n",
      "4344\n",
      "4821\n",
      "5470\n",
      "5894\n",
      "6156\n",
      "6275\n",
      "6732\n",
      "7156\n",
      "7633\n",
      "8130\n",
      "8554\n",
      "8978\n",
      "9647\n",
      "9997\n",
      "10116\n",
      "10378\n",
      "10620\n",
      "11120\n",
      "11597\n",
      "12048\n",
      "12187\n",
      "12306\n",
      "12445\n",
      "12869\n",
      "13292\n",
      "13411\n",
      "13868\n",
      "13987\n",
      "14106\n",
      "14265\n",
      "14384\n",
      "已收集到 38 个帖子\n",
      "8978\n",
      "9647\n",
      "9997\n",
      "10116\n",
      "10378\n",
      "10620\n",
      "11120\n",
      "11597\n",
      "12048\n",
      "12187\n",
      "12306\n",
      "12445\n",
      "12869\n",
      "13292\n",
      "13411\n",
      "13868\n",
      "13987\n",
      "14106\n",
      "14265\n",
      "14384\n",
      "14786\n",
      "15210\n",
      "15634\n",
      "16057\n",
      "16461\n",
      "16978\n",
      "17117\n",
      "17379\n",
      "17839\n",
      "18101\n",
      "18220\n",
      "18664\n",
      "19088\n",
      "19227\n",
      "已收集到 52 个帖子\n",
      "13411\n",
      "13868\n",
      "13987\n",
      "14106\n",
      "14265\n",
      "14384\n",
      "14786\n",
      "15210\n",
      "15634\n",
      "16057\n",
      "16461\n",
      "16978\n",
      "17117\n",
      "17379\n",
      "17839\n",
      "18101\n",
      "18220\n",
      "18664\n",
      "19088\n",
      "19227\n",
      "19651\n",
      "19770\n",
      "20302\n",
      "20726\n",
      "21150\n",
      "21594\n",
      "22091\n",
      "22210\n",
      "22707\n",
      "22826\n",
      "23250\n",
      "23695\n",
      "已收集到 64 个帖子\n",
      "18220\n",
      "18664\n",
      "19088\n",
      "19227\n",
      "19651\n",
      "19770\n",
      "20302\n",
      "20726\n",
      "21150\n",
      "21594\n",
      "22091\n",
      "22210\n",
      "22707\n",
      "22826\n",
      "23250\n",
      "23695\n",
      "23977\n",
      "24096\n",
      "24604\n",
      "24723\n",
      "25147\n",
      "25266\n",
      "25929\n",
      "26407\n",
      "26566\n",
      "26665\n",
      "27088\n",
      "27207\n",
      "27631\n",
      "27750\n",
      "已收集到 77 个帖子\n",
      "22707\n",
      "22826\n",
      "23250\n",
      "23695\n",
      "23977\n",
      "24096\n",
      "24604\n",
      "24723\n",
      "25147\n",
      "25266\n",
      "25929\n",
      "26407\n",
      "26566\n",
      "26665\n",
      "27088\n",
      "27207\n",
      "27631\n",
      "27750\n",
      "28191\n",
      "28691\n",
      "29377\n",
      "29801\n",
      "30225\n",
      "30649\n",
      "30788\n",
      "31265\n",
      "31689\n",
      "32133\n",
      "32548\n",
      "已收集到 88 个帖子\n",
      "27207\n",
      "27631\n",
      "27750\n",
      "28191\n",
      "28691\n",
      "29377\n",
      "29801\n",
      "30225\n",
      "30649\n",
      "30788\n",
      "31265\n",
      "31689\n",
      "32133\n",
      "32548\n",
      "33108\n",
      "33592\n",
      "已收集到 89 个帖子\n",
      "31689\n",
      "32133\n",
      "32548\n",
      "33108\n",
      "33592\n",
      "34092\n",
      "34516\n",
      "已收集到 90 个帖子\n",
      "36298\n",
      "36523\n",
      "37004\n",
      "37103\n",
      "37222\n",
      "37381\n",
      "37520\n",
      "37639\n",
      "38262\n",
      "已收集到 98 个帖子\n",
      "40631\n",
      "已收集到 98 个帖子\n",
      "45234\n",
      "已收集到 98 个帖子\n",
      "49769\n",
      "49928\n",
      "50047\n",
      "50146\n",
      "已收集到 101 个帖子\n",
      "50265\n",
      "50772\n",
      "已收集到 102 个帖子\n",
      "50265\n",
      "50772\n",
      "50891\n",
      "51010\n",
      "已收集到 103 个帖子\n",
      "50265\n",
      "50772\n",
      "50891\n",
      "51010\n",
      "51129\n",
      "已收集到 103 个帖子\n",
      "50265\n",
      "50772\n",
      "50891\n",
      "51010\n",
      "51129\n",
      "51248\n",
      "已收集到 103 个帖子\n",
      "50265\n",
      "50772\n",
      "50891\n",
      "51010\n",
      "51129\n",
      "51248\n",
      "51367\n",
      "已收集到 103 个帖子\n",
      "50265\n",
      "50772\n",
      "50891\n",
      "51010\n",
      "51129\n",
      "51248\n",
      "51367\n",
      "51486\n",
      "已收集到 103 个帖子\n",
      "50265\n",
      "50772\n",
      "50891\n",
      "51010\n",
      "51129\n",
      "51248\n",
      "51367\n",
      "51486\n",
      "51605\n",
      "已收集到 103 个帖子\n",
      "多次滚动后没有新数据，等60秒\n",
      "50265\n",
      "50772\n",
      "50891\n",
      "51010\n",
      "51129\n",
      "51248\n",
      "51367\n",
      "51486\n",
      "51605\n",
      "51744\n",
      "已收集到 103 个帖子\n",
      "多次滚动后没有新数据，已尝试 6\n",
      "停止爬取。\n",
      "爬取日期：2017-12-31~2018-12-31\n",
      "0\n",
      "0\n",
      "48\n",
      "158\n",
      "210\n",
      "219\n",
      "908\n",
      "1737\n",
      "2201\n",
      "2781\n",
      "3305\n",
      "3424\n",
      "4004\n",
      "4441\n",
      "5021\n",
      "5303\n",
      "5585\n",
      "已收集到 11 个帖子\n",
      "219\n",
      "908\n",
      "1737\n",
      "2201\n",
      "2781\n",
      "3305\n",
      "3424\n",
      "4004\n",
      "4441\n",
      "5021\n",
      "5303\n",
      "5585\n",
      "6039\n",
      "6486\n",
      "6930\n",
      "7354\n",
      "7758\n",
      "7897\n",
      "8620\n",
      "8865\n",
      "9422\n",
      "9541\n",
      "9680\n",
      "10303\n",
      "已收集到 23 个帖子\n",
      "4441\n",
      "5021\n",
      "5303\n",
      "5585\n",
      "6039\n",
      "6486\n",
      "6930\n",
      "7354\n",
      "7758\n",
      "7897\n",
      "8620\n",
      "8865\n",
      "9422\n",
      "9541\n",
      "9680\n",
      "10303\n",
      "11006\n",
      "11308\n",
      "11752\n",
      "12196\n",
      "12620\n",
      "13018\n",
      "13551\n",
      "13670\n",
      "13829\n",
      "14293\n",
      "14394\n",
      "14838\n",
      "已收集到 35 个帖子\n",
      "8865\n",
      "9422\n",
      "9541\n",
      "9680\n",
      "10303\n",
      "11006\n",
      "11308\n",
      "11752\n",
      "12196\n",
      "12620\n",
      "13018\n",
      "13551\n",
      "13670\n",
      "13829\n",
      "14293\n",
      "14394\n",
      "14838\n",
      "15260\n",
      "15419\n",
      "16222\n",
      "16521\n",
      "16680\n",
      "17346\n",
      "17790\n",
      "18294\n",
      "18738\n",
      "18897\n",
      "已收集到 45 个帖子\n",
      "13670\n",
      "13829\n",
      "14293\n",
      "14394\n",
      "14838\n",
      "15260\n",
      "15419\n",
      "16222\n",
      "16521\n",
      "16680\n",
      "17346\n",
      "17790\n",
      "18294\n",
      "18738\n",
      "18897\n",
      "19603\n",
      "20067\n",
      "20531\n",
      "21277\n",
      "21436\n",
      "21761\n",
      "22110\n",
      "22269\n",
      "22872\n",
      "23316\n",
      "已收集到 55 个帖子\n",
      "18294\n",
      "18738\n",
      "18897\n",
      "19603\n",
      "20067\n",
      "20531\n",
      "21277\n",
      "21436\n",
      "21761\n",
      "22110\n",
      "22269\n",
      "22872\n",
      "23316\n",
      "24002\n",
      "24614\n",
      "24931\n",
      "25090\n",
      "25349\n",
      "26032\n",
      "26171\n",
      "26350\n",
      "26794\n",
      "27225\n",
      "27527\n",
      "27971\n",
      "已收集到 67 个帖子\n",
      "22269\n",
      "22872\n",
      "23316\n",
      "24002\n",
      "24614\n",
      "24931\n",
      "25090\n",
      "25349\n",
      "26032\n",
      "26171\n",
      "26350\n",
      "26794\n",
      "27225\n",
      "27527\n",
      "27971\n",
      "28492\n",
      "28631\n",
      "28770\n",
      "28929\n",
      "29214\n",
      "29353\n",
      "29492\n",
      "29651\n",
      "30142\n",
      "30301\n",
      "30765\n",
      "30924\n",
      "已收集到 78 个帖子\n",
      "27225\n",
      "27527\n",
      "27971\n",
      "28492\n",
      "28631\n",
      "28770\n",
      "28929\n",
      "29214\n",
      "29353\n",
      "29492\n",
      "29651\n",
      "30142\n",
      "30301\n",
      "30765\n",
      "30924\n",
      "31043\n",
      "31261\n",
      "31460\n",
      "31619\n",
      "32282\n",
      "32481\n",
      "33061\n",
      "已收集到 84 个帖子\n",
      "31619\n",
      "32282\n",
      "32481\n",
      "33061\n",
      "33475\n",
      "33919\n",
      "34078\n",
      "34237\n",
      "34356\n",
      "35082\n",
      "35201\n",
      "35685\n",
      "36388\n",
      "36567\n",
      "36766\n",
      "36885\n",
      "37369\n",
      "37508\n",
      "37647\n",
      "37806\n",
      "已收集到 99 个帖子\n",
      "35685\n",
      "36388\n",
      "36567\n",
      "36766\n",
      "36885\n",
      "37369\n",
      "37508\n",
      "37647\n",
      "37806\n",
      "38529\n",
      "38728\n",
      "39131\n",
      "39270\n",
      "39432\n",
      "已收集到 103 个帖子\n",
      "40515\n",
      "已收集到 103 个帖子\n",
      "45284\n",
      "45462\n",
      "45700\n",
      "已收集到 105 个帖子\n",
      "49788\n",
      "已收集到 105 个帖子\n",
      "51083\n",
      "已收集到 105 个帖子\n",
      "51083\n",
      "51202\n",
      "已收集到 105 个帖子\n",
      "51083\n",
      "51202\n",
      "51341\n",
      "已收集到 105 个帖子\n",
      "51083\n",
      "51202\n",
      "51341\n",
      "51500\n",
      "已收集到 105 个帖子\n",
      "多次滚动后没有新数据，等60秒\n",
      "51083\n",
      "51202\n",
      "51341\n",
      "51500\n",
      "51619\n",
      "已收集到 105 个帖子\n",
      "多次滚动后没有新数据，已尝试 6\n",
      "停止爬取。\n",
      "爬取日期：2018-12-31~2019-12-31\n",
      "0\n",
      "0\n",
      "48\n",
      "158\n",
      "210\n",
      "219\n",
      "870\n",
      "1297\n",
      "1917\n",
      "2095\n",
      "已收集到 3 个帖子\n",
      "219\n",
      "870\n",
      "1297\n",
      "1917\n",
      "2095\n",
      "2764\n",
      "3248\n",
      "3387\n",
      "3692\n",
      "4136\n",
      "4643\n",
      "4945\n",
      "5389\n",
      "5608\n",
      "5747\n",
      "5886\n",
      "6065\n",
      "6327\n",
      "已收集到 15 个帖子\n",
      "4643\n",
      "4945\n",
      "5389\n",
      "5608\n",
      "5747\n",
      "5886\n",
      "6065\n",
      "6327\n",
      "6995\n",
      "7174\n",
      "已收集到 16 个帖子\n",
      "9192\n",
      "9597\n",
      "10061\n",
      "10180\n",
      "10462\n",
      "10601\n",
      "11344\n",
      "12167\n",
      "12611\n",
      "13035\n",
      "13674\n",
      "14118\n",
      "14801\n",
      "15285\n",
      "15749\n",
      "15888\n",
      "16557\n",
      "16696\n",
      "已收集到 33 个帖子\n",
      "13674\n",
      "14118\n",
      "14801\n",
      "15285\n",
      "15749\n",
      "15888\n",
      "16557\n",
      "16696\n",
      "17405\n",
      "17760\n",
      "18204\n",
      "18343\n",
      "18482\n",
      "18641\n",
      "18823\n",
      "19267\n",
      "19426\n",
      "已收集到 41 个帖子\n",
      "18204\n",
      "18343\n",
      "18482\n",
      "18641\n",
      "18823\n",
      "19267\n",
      "19426\n",
      "19850\n",
      "已收集到 41 个帖子\n",
      "22391\n",
      "22802\n",
      "23124\n",
      "23807\n",
      "已收集到 44 个帖子\n",
      "27119\n",
      "27318\n",
      "28054\n",
      "28555\n",
      "28694\n",
      "28833\n",
      "29256\n",
      "29395\n",
      "29574\n",
      "已收集到 52 个帖子\n",
      "31475\n",
      "31614\n",
      "31773\n",
      "31932\n",
      "已收集到 55 个帖子\n",
      "35741\n",
      "36246\n",
      "36405\n",
      "已收集到 57 个帖子\n",
      "40536\n",
      "41000\n",
      "41179\n",
      "已收集到 59 个帖子\n",
      "45014\n",
      "45173\n",
      "45352\n",
      "已收集到 61 个帖子\n",
      "49268\n",
      "已收集到 61 个帖子\n",
      "52541\n",
      "已收集到 61 个帖子\n",
      "52541\n",
      "52870\n",
      "已收集到 61 个帖子\n",
      "52541\n",
      "52870\n",
      "53155\n",
      "53294\n",
      "已收集到 62 个帖子\n",
      "52541\n",
      "52870\n",
      "53155\n",
      "53294\n",
      "53563\n",
      "已收集到 62 个帖子\n",
      "52541\n",
      "52870\n",
      "53155\n",
      "53294\n",
      "53563\n",
      "53722\n",
      "已收集到 62 个帖子\n",
      "52541\n",
      "52870\n",
      "53155\n",
      "53294\n",
      "53563\n",
      "53722\n",
      "53841\n",
      "已收集到 62 个帖子\n",
      "52541\n",
      "52870\n",
      "53155\n",
      "53294\n",
      "53563\n",
      "53722\n",
      "53841\n",
      "54000\n",
      "54159\n",
      "已收集到 63 个帖子\n",
      "52541\n",
      "52870\n",
      "53155\n",
      "53294\n",
      "53563\n",
      "53722\n",
      "53841\n",
      "54000\n",
      "54159\n",
      "54278\n",
      "已收集到 63 个帖子\n",
      "52541\n",
      "52870\n",
      "53155\n",
      "53294\n",
      "53563\n",
      "53722\n",
      "53841\n",
      "54000\n",
      "54159\n",
      "54278\n",
      "54397\n",
      "已收集到 63 个帖子\n",
      "52541\n",
      "52870\n",
      "53155\n",
      "53294\n",
      "53563\n",
      "53722\n",
      "53841\n",
      "54000\n",
      "54159\n",
      "54278\n",
      "54397\n",
      "54556\n",
      "54715\n",
      "已收集到 64 个帖子\n",
      "52541\n",
      "52870\n",
      "53155\n",
      "53294\n",
      "53563\n",
      "53722\n",
      "53841\n",
      "54000\n",
      "54159\n",
      "54278\n",
      "54397\n",
      "54556\n",
      "54715\n",
      "54974\n",
      "已收集到 64 个帖子\n",
      "52541\n",
      "52870\n",
      "53155\n",
      "53294\n",
      "53563\n",
      "53722\n",
      "53841\n",
      "54000\n",
      "54159\n",
      "54278\n",
      "54397\n",
      "54556\n",
      "54715\n",
      "54974\n",
      "55153\n",
      "已收集到 64 个帖子\n",
      "52541\n",
      "52870\n",
      "53155\n",
      "53294\n",
      "53563\n",
      "53722\n",
      "53841\n",
      "54000\n",
      "54159\n",
      "54278\n",
      "54397\n",
      "54556\n",
      "54715\n",
      "54974\n",
      "55153\n",
      "55352\n",
      "55531\n",
      "已收集到 65 个帖子\n",
      "52541\n",
      "52870\n",
      "53155\n",
      "53294\n",
      "53563\n",
      "53722\n",
      "53841\n",
      "54000\n",
      "54159\n",
      "54278\n",
      "54397\n",
      "54556\n",
      "54715\n",
      "54974\n",
      "55153\n",
      "55352\n",
      "55531\n",
      "55690\n",
      "已收集到 65 个帖子\n",
      "52541\n",
      "52870\n",
      "53155\n",
      "53294\n",
      "53563\n",
      "53722\n",
      "53841\n",
      "54000\n",
      "54159\n",
      "54278\n",
      "54397\n",
      "54556\n",
      "54715\n",
      "54974\n",
      "55153\n",
      "55352\n",
      "55531\n",
      "55690\n",
      "55888\n",
      "56428\n",
      "已收集到 66 个帖子\n",
      "52541\n",
      "52870\n",
      "53155\n",
      "53294\n",
      "53563\n",
      "53722\n",
      "53841\n",
      "54000\n",
      "54159\n",
      "54278\n",
      "54397\n",
      "54556\n",
      "54715\n",
      "54974\n",
      "55153\n",
      "55352\n",
      "55531\n",
      "55690\n",
      "55888\n",
      "56428\n",
      "56547\n",
      "56666\n",
      "56785\n",
      "已收集到 68 个帖子\n",
      "52541\n",
      "52870\n",
      "53155\n",
      "53294\n",
      "53563\n",
      "53722\n",
      "53841\n",
      "54000\n",
      "54159\n",
      "54278\n",
      "54397\n",
      "54556\n",
      "54715\n",
      "54974\n",
      "55153\n",
      "55352\n",
      "55531\n",
      "55690\n",
      "55888\n",
      "56428\n",
      "56547\n",
      "56666\n",
      "56785\n",
      "56904\n",
      "已收集到 68 个帖子\n",
      "52541\n",
      "52870\n",
      "53155\n",
      "53294\n",
      "53563\n",
      "53722\n",
      "53841\n",
      "54000\n",
      "54159\n",
      "54278\n",
      "54397\n",
      "54556\n",
      "54715\n",
      "54974\n",
      "55153\n",
      "55352\n",
      "55531\n",
      "55690\n",
      "55888\n",
      "56428\n",
      "56547\n",
      "56666\n",
      "56785\n",
      "56904\n",
      "已收集到 68 个帖子\n",
      "52541\n",
      "52870\n",
      "53155\n",
      "53294\n",
      "53563\n",
      "53722\n",
      "53841\n",
      "54000\n",
      "54159\n",
      "54278\n",
      "54397\n",
      "54556\n",
      "54715\n",
      "54974\n",
      "55153\n",
      "55352\n",
      "55531\n",
      "55690\n",
      "55888\n",
      "56428\n",
      "56547\n",
      "56666\n",
      "56785\n",
      "56904\n",
      "已收集到 68 个帖子\n",
      "52541\n",
      "52870\n",
      "53155\n",
      "53294\n",
      "53563\n",
      "53722\n",
      "53841\n",
      "54000\n",
      "54159\n",
      "54278\n",
      "54397\n",
      "54556\n",
      "54715\n",
      "54974\n",
      "55153\n",
      "55352\n",
      "55531\n",
      "55690\n",
      "55888\n",
      "56428\n",
      "56547\n",
      "56666\n",
      "56785\n",
      "56904\n",
      "已收集到 68 个帖子\n",
      "52541\n",
      "52870\n",
      "53155\n",
      "53294\n",
      "53563\n",
      "53722\n",
      "53841\n",
      "54000\n",
      "54159\n",
      "54278\n",
      "54397\n",
      "54556\n",
      "54715\n",
      "54974\n",
      "55153\n",
      "55352\n",
      "55531\n",
      "55690\n",
      "55888\n",
      "56428\n",
      "56547\n",
      "56666\n",
      "56785\n",
      "56904\n",
      "已收集到 68 个帖子\n",
      "多次滚动后没有新数据，等60秒\n",
      "52541\n",
      "52870\n",
      "53155\n",
      "53294\n",
      "53563\n",
      "53722\n",
      "53841\n",
      "54000\n",
      "54159\n",
      "54278\n",
      "54397\n",
      "54556\n",
      "54715\n",
      "54974\n",
      "55153\n",
      "55352\n",
      "55531\n",
      "55690\n",
      "55888\n",
      "56428\n",
      "56547\n",
      "56666\n",
      "56785\n",
      "56904\n",
      "已收集到 68 个帖子\n",
      "多次滚动后没有新数据，已尝试 6\n",
      "停止爬取。\n",
      "爬取日期：2019-12-31~2020-12-30\n",
      "0\n",
      "0\n",
      "48\n",
      "158\n",
      "210\n",
      "219\n",
      "723\n",
      "1374\n",
      "2025\n",
      "2549\n",
      "3200\n",
      "3676\n",
      "已收集到 5 个帖子\n",
      "219\n",
      "723\n",
      "1374\n",
      "2025\n",
      "2549\n",
      "3200\n",
      "3676\n",
      "4205\n",
      "已收集到 5 个帖子\n",
      "4689\n",
      "5146\n",
      "5550\n",
      "6027\n",
      "6504\n",
      "6981\n",
      "7140\n",
      "7422\n",
      "7899\n",
      "8679\n",
      "9156\n",
      "9633\n",
      "10077\n",
      "10806\n",
      "11423\n",
      "11562\n",
      "12027\n",
      "12148\n",
      "12794\n",
      "13442\n",
      "13601\n",
      "14218\n",
      "14691\n",
      "已收集到 28 个帖子\n",
      "9156\n",
      "9633\n",
      "10077\n",
      "10806\n",
      "11423\n",
      "11562\n",
      "12027\n",
      "12148\n",
      "12794\n",
      "13442\n",
      "13601\n",
      "14218\n",
      "14691\n",
      "15168\n",
      "15752\n",
      "16154\n",
      "16817\n",
      "16956\n",
      "17785\n",
      "18289\n",
      "18952\n",
      "已收集到 36 个帖子\n",
      "13601\n",
      "14218\n",
      "14691\n",
      "15168\n",
      "15752\n",
      "16154\n",
      "16817\n",
      "16956\n",
      "17785\n",
      "18289\n",
      "18952\n",
      "19456\n",
      "20107\n",
      "20715\n",
      "21166\n",
      "21671\n",
      "22334\n",
      "22818\n",
      "23322\n",
      "23746\n",
      "已收集到 45 个帖子\n",
      "18289\n",
      "18952\n",
      "19456\n",
      "20107\n",
      "20715\n",
      "21166\n",
      "21671\n",
      "22334\n",
      "22818\n",
      "23322\n",
      "23746\n",
      "24286\n",
      "已收集到 45 个帖子\n",
      "22334\n",
      "22818\n",
      "23322\n",
      "23746\n",
      "24286\n",
      "24730\n",
      "25174\n",
      "25778\n",
      "26637\n",
      "27121\n",
      "27280\n",
      "已收集到 50 个帖子\n",
      "27280\n",
      "27751\n",
      "27890\n",
      "28481\n",
      "28680\n",
      "28919\n",
      "29158\n",
      "已收集到 55 个帖子\n",
      "31595\n",
      "31897\n",
      "32056\n",
      "32765\n",
      "33325\n",
      "33750\n",
      "34390\n",
      "34628\n",
      "34787\n",
      "35025\n",
      "35290\n",
      "35449\n",
      "35648\n",
      "35827\n",
      "36466\n",
      "已收集到 69 个帖子\n",
      "35827\n",
      "36466\n",
      "36937\n",
      "已收集到 69 个帖子\n",
      "40436\n",
      "40940\n",
      "已收集到 70 个帖子\n",
      "45271\n",
      "45536\n",
      "45695\n",
      "45834\n",
      "45993\n",
      "46152\n",
      "已收集到 75 个帖子\n",
      "49592\n",
      "49914\n",
      "50159\n",
      "已收集到 77 个帖子\n",
      "54134\n",
      "54316\n",
      "已收集到 78 个帖子\n",
      "55829\n",
      "已收集到 78 个帖子\n",
      "56751\n",
      "已收集到 78 个帖子\n",
      "56751\n",
      "57050\n",
      "已收集到 78 个帖子\n",
      "56751\n",
      "57050\n",
      "57209\n",
      "57328\n",
      "已收集到 79 个帖子\n",
      "56751\n",
      "57050\n",
      "57209\n",
      "57328\n",
      "57467\n",
      "已收集到 79 个帖子\n",
      "56751\n",
      "57050\n",
      "57209\n",
      "57328\n",
      "57467\n",
      "57566\n",
      "57728\n",
      "已收集到 80 个帖子\n",
      "56751\n",
      "57050\n",
      "57209\n",
      "57328\n",
      "57467\n",
      "57566\n",
      "57728\n",
      "57847\n",
      "已收集到 80 个帖子\n",
      "56751\n",
      "57050\n",
      "57209\n",
      "57328\n",
      "57467\n",
      "57566\n",
      "57728\n",
      "57847\n",
      "58032\n",
      "已收集到 80 个帖子\n",
      "56751\n",
      "57050\n",
      "57209\n",
      "57328\n",
      "57467\n",
      "57566\n",
      "57728\n",
      "57847\n",
      "58032\n",
      "58747\n",
      "已收集到 80 个帖子\n",
      "56751\n",
      "57050\n",
      "57209\n",
      "57328\n",
      "57467\n",
      "57566\n",
      "57728\n",
      "57847\n",
      "58032\n",
      "58747\n",
      "58906\n",
      "59085\n",
      "已收集到 81 个帖子\n",
      "56751\n",
      "57050\n",
      "57209\n",
      "57328\n",
      "57467\n",
      "57566\n",
      "57728\n",
      "57847\n",
      "58032\n",
      "58747\n",
      "58906\n",
      "59085\n",
      "59243\n",
      "已收集到 81 个帖子\n",
      "56751\n",
      "57050\n",
      "57209\n",
      "57328\n",
      "57467\n",
      "57566\n",
      "57728\n",
      "57847\n",
      "58032\n",
      "58747\n",
      "58906\n",
      "59085\n",
      "59243\n",
      "59342\n",
      "已收集到 81 个帖子\n",
      "56751\n",
      "57050\n",
      "57209\n",
      "57328\n",
      "57467\n",
      "57566\n",
      "57728\n",
      "57847\n",
      "58032\n",
      "58747\n",
      "58906\n",
      "59085\n",
      "59243\n",
      "59342\n",
      "59501\n",
      "已收集到 81 个帖子\n",
      "56751\n",
      "57050\n",
      "57209\n",
      "57328\n",
      "57467\n",
      "57566\n",
      "57728\n",
      "57847\n",
      "58032\n",
      "58747\n",
      "58906\n",
      "59085\n",
      "59243\n",
      "59342\n",
      "59501\n",
      "59766\n",
      "已收集到 81 个帖子\n",
      "56751\n",
      "57050\n",
      "57209\n",
      "57328\n",
      "57467\n",
      "57566\n",
      "57728\n",
      "57847\n",
      "58032\n",
      "58747\n",
      "58906\n",
      "59085\n",
      "59243\n",
      "59342\n",
      "59501\n",
      "59766\n",
      "59885\n",
      "60044\n",
      "已收集到 82 个帖子\n",
      "56751\n",
      "57050\n",
      "57209\n",
      "57328\n",
      "57467\n",
      "57566\n",
      "57728\n",
      "57847\n",
      "58032\n",
      "58747\n",
      "58906\n",
      "59085\n",
      "59243\n",
      "59342\n",
      "59501\n",
      "59766\n",
      "59885\n",
      "60044\n",
      "60163\n",
      "已收集到 82 个帖子\n",
      "56751\n",
      "57050\n",
      "57209\n",
      "57328\n",
      "57467\n",
      "57566\n",
      "57728\n",
      "57847\n",
      "58032\n",
      "58747\n",
      "58906\n",
      "59085\n",
      "59243\n",
      "59342\n",
      "59501\n",
      "59766\n",
      "59885\n",
      "60044\n",
      "60163\n",
      "60721\n",
      "已收集到 82 个帖子\n",
      "56751\n",
      "57050\n",
      "57209\n",
      "57328\n",
      "57467\n",
      "57566\n",
      "57728\n",
      "57847\n",
      "58032\n",
      "58747\n",
      "58906\n",
      "59085\n",
      "59243\n",
      "59342\n",
      "59501\n",
      "59766\n",
      "59885\n",
      "60044\n",
      "60163\n",
      "60721\n",
      "61001\n",
      "61263\n",
      "已收集到 83 个帖子\n",
      "56751\n",
      "57050\n",
      "57209\n",
      "57328\n",
      "57467\n",
      "57566\n",
      "57728\n",
      "57847\n",
      "58032\n",
      "58747\n",
      "58906\n",
      "59085\n",
      "59243\n",
      "59342\n",
      "59501\n",
      "59766\n",
      "59885\n",
      "60044\n",
      "60163\n",
      "60721\n",
      "61001\n",
      "61263\n",
      "已收集到 83 个帖子\n",
      "56751\n",
      "57050\n",
      "57209\n",
      "57328\n",
      "57467\n",
      "57566\n",
      "57728\n",
      "57847\n",
      "58032\n",
      "58747\n",
      "58906\n",
      "59085\n",
      "59243\n",
      "59342\n",
      "59501\n",
      "59766\n",
      "59885\n",
      "60044\n",
      "60163\n",
      "60721\n",
      "61001\n",
      "61263\n",
      "已收集到 83 个帖子\n",
      "56751\n",
      "57050\n",
      "57209\n",
      "57328\n",
      "57467\n",
      "57566\n",
      "57728\n",
      "57847\n",
      "58032\n",
      "58747\n",
      "58906\n",
      "59085\n",
      "59243\n",
      "59342\n",
      "59501\n",
      "59766\n",
      "59885\n",
      "60044\n",
      "60163\n",
      "60721\n",
      "61001\n",
      "61263\n",
      "已收集到 83 个帖子\n",
      "56751\n",
      "57050\n",
      "57209\n",
      "57328\n",
      "57467\n",
      "57566\n",
      "57728\n",
      "57847\n",
      "58032\n",
      "58747\n",
      "58906\n",
      "59085\n",
      "59243\n",
      "59342\n",
      "59501\n",
      "59766\n",
      "59885\n",
      "60044\n",
      "60163\n",
      "60721\n",
      "61001\n",
      "61263\n",
      "已收集到 83 个帖子\n",
      "56751\n",
      "57050\n",
      "57209\n",
      "57328\n",
      "57467\n",
      "57566\n",
      "57728\n",
      "57847\n",
      "58032\n",
      "58747\n",
      "58906\n",
      "59085\n",
      "59243\n",
      "59342\n",
      "59501\n",
      "59766\n",
      "59885\n",
      "60044\n",
      "60163\n",
      "60721\n",
      "61001\n",
      "61263\n",
      "已收集到 83 个帖子\n",
      "多次滚动后没有新数据，等60秒\n",
      "56751\n",
      "57050\n",
      "57209\n",
      "57328\n",
      "57467\n",
      "57566\n",
      "57728\n",
      "57847\n",
      "58032\n",
      "58747\n",
      "58906\n",
      "59085\n",
      "59243\n",
      "59342\n",
      "59501\n",
      "59766\n",
      "59885\n",
      "60044\n",
      "60163\n",
      "60721\n",
      "61001\n",
      "61263\n",
      "已收集到 83 个帖子\n",
      "多次滚动后没有新数据，已尝试 6\n",
      "停止爬取。\n",
      "爬取日期：2020-12-30~2021-12-30\n",
      "0\n",
      "0\n",
      "48\n",
      "158\n",
      "210\n",
      "219\n",
      "643\n",
      "1143\n",
      "已收集到 1 个帖子\n",
      "219\n",
      "643\n",
      "1143\n",
      "1932\n",
      "已收集到 1 个帖子\n",
      "4121\n",
      "4810\n",
      "4989\n",
      "5433\n",
      "6196\n",
      "已收集到 5 个帖子\n",
      "9014\n",
      "9458\n",
      "9577\n",
      "10266\n",
      "10995\n",
      "11535\n",
      "11999\n",
      "12463\n",
      "12960\n",
      "13788\n",
      "14325\n",
      "14543\n",
      "15250\n",
      "15933\n",
      "16654\n",
      "17118\n",
      "17702\n",
      "18186\n",
      "18345\n",
      "18771\n",
      "19180\n",
      "已收集到 26 个帖子\n",
      "13788\n",
      "14325\n",
      "14543\n",
      "15250\n",
      "15933\n",
      "16654\n",
      "17118\n",
      "17702\n",
      "18186\n",
      "18345\n",
      "18771\n",
      "19180\n",
      "19624\n",
      "19969\n",
      "20652\n",
      "20891\n",
      "21173\n",
      "21597\n",
      "21879\n",
      "22303\n",
      "已收集到 33 个帖子\n",
      "18186\n",
      "18345\n",
      "18771\n",
      "19180\n",
      "19624\n",
      "19969\n",
      "20652\n",
      "20891\n",
      "21173\n",
      "21597\n",
      "21879\n",
      "22303\n",
      "23020\n",
      "23159\n",
      "23659\n",
      "23921\n",
      "24569\n",
      "25214\n",
      "25798\n",
      "26458\n",
      "26946\n",
      "27528\n",
      "27810\n",
      "27969\n",
      "28271\n",
      "已收集到 46 个帖子\n",
      "22303\n",
      "23020\n",
      "23159\n",
      "23659\n",
      "23921\n",
      "24569\n",
      "25214\n",
      "25798\n",
      "26458\n",
      "26946\n",
      "27528\n",
      "27810\n",
      "27969\n",
      "28271\n",
      "28470\n",
      "29153\n",
      "已收集到 47 个帖子\n",
      "26946\n",
      "27528\n",
      "27810\n",
      "27969\n",
      "28271\n",
      "28470\n",
      "29153\n",
      "29488\n",
      "29627\n",
      "29766\n",
      "29945\n",
      "已收集到 50 个帖子\n",
      "31753\n",
      "31892\n",
      "32011\n",
      "已收集到 52 个帖子\n",
      "36200\n",
      "已收集到 52 个帖子\n",
      "40588\n",
      "已收集到 52 个帖子\n",
      "45198\n",
      "45317\n",
      "45456\n",
      "45638\n",
      "已收集到 55 个帖子\n",
      "45456\n",
      "45638\n",
      "46287\n",
      "46486\n",
      "已收集到 56 个帖子\n",
      "45456\n",
      "45638\n",
      "46287\n",
      "46486\n",
      "46605\n",
      "已收集到 56 个帖子\n",
      "45456\n",
      "45638\n",
      "46287\n",
      "46486\n",
      "46605\n",
      "46907\n",
      "47046\n",
      "47265\n",
      "47424\n",
      "47563\n",
      "47722\n",
      "47841\n",
      "48000\n",
      "48182\n",
      "48504\n",
      "48686\n",
      "48848\n",
      "49010\n",
      "49169\n",
      "49291\n",
      "49533\n",
      "49735\n",
      "49957\n",
      "已收集到 73 个帖子\n",
      "45456\n",
      "45638\n",
      "46287\n",
      "46486\n",
      "46605\n",
      "46907\n",
      "47046\n",
      "47265\n",
      "47424\n",
      "47563\n",
      "47722\n",
      "47841\n",
      "48000\n",
      "48182\n",
      "48504\n",
      "48686\n",
      "48848\n",
      "49010\n",
      "49169\n",
      "49291\n",
      "49533\n",
      "49735\n",
      "49957\n",
      "已收集到 73 个帖子\n",
      "45456\n",
      "45638\n",
      "46287\n",
      "46486\n",
      "46605\n",
      "46907\n",
      "47046\n",
      "47265\n",
      "47424\n",
      "47563\n",
      "47722\n",
      "47841\n",
      "48000\n",
      "48182\n",
      "48504\n",
      "48686\n",
      "48848\n",
      "49010\n",
      "49169\n",
      "49291\n",
      "49533\n",
      "49735\n",
      "49957\n",
      "已收集到 73 个帖子\n",
      "45456\n",
      "45638\n",
      "46287\n",
      "46486\n",
      "46605\n",
      "46907\n",
      "47046\n",
      "47265\n",
      "47424\n",
      "47563\n",
      "47722\n",
      "47841\n",
      "48000\n",
      "48182\n",
      "48504\n",
      "48686\n",
      "48848\n",
      "49010\n",
      "49169\n",
      "49291\n",
      "49533\n",
      "49735\n",
      "49957\n",
      "已收集到 73 个帖子\n",
      "45456\n",
      "45638\n",
      "46287\n",
      "46486\n",
      "46605\n",
      "46907\n",
      "47046\n",
      "47265\n",
      "47424\n",
      "47563\n",
      "47722\n",
      "47841\n",
      "48000\n",
      "48182\n",
      "48504\n",
      "48686\n",
      "48848\n",
      "49010\n",
      "49169\n",
      "49291\n",
      "49533\n",
      "49735\n",
      "49957\n",
      "已收集到 73 个帖子\n",
      "45456\n",
      "45638\n",
      "46287\n",
      "46486\n",
      "46605\n",
      "46907\n",
      "47046\n",
      "47265\n",
      "47424\n",
      "47563\n",
      "47722\n",
      "47841\n",
      "48000\n",
      "48182\n",
      "48504\n",
      "48686\n",
      "48848\n",
      "49010\n",
      "49169\n",
      "49291\n",
      "49533\n",
      "49735\n",
      "49957\n",
      "已收集到 73 个帖子\n",
      "检测到'Retry'按钮，当前时间: 2025-02-18 23:48:36,滚动了19次\n",
      "等待6分钟...\n",
      "点击'Retry'按钮成功，等待5秒后继续爬取，当前时间: 2025-02-18 23:54:36\n",
      "多次滚动后没有新数据，等60秒\n",
      "49957\n",
      "50076\n",
      "50215\n",
      "50354\n",
      "50493\n",
      "已收集到 76 个帖子\n",
      "54380\n",
      "已收集到 76 个帖子\n",
      "55969\n",
      "已收集到 76 个帖子\n",
      "55969\n",
      "56489\n",
      "56668\n",
      "56847\n",
      "57029\n",
      "57168\n",
      "57987\n",
      "已收集到 81 个帖子\n",
      "55969\n",
      "56489\n",
      "56668\n",
      "56847\n",
      "57029\n",
      "57168\n",
      "57987\n",
      "58106\n",
      "已收集到 81 个帖子\n",
      "55969\n",
      "56489\n",
      "56668\n",
      "56847\n",
      "57029\n",
      "57168\n",
      "57987\n",
      "58106\n",
      "58225\n",
      "58384\n",
      "58503\n",
      "58602\n",
      "58761\n",
      "已收集到 85 个帖子\n",
      "55969\n",
      "56489\n",
      "56668\n",
      "56847\n",
      "57029\n",
      "57168\n",
      "57987\n",
      "58106\n",
      "58225\n",
      "58384\n",
      "58503\n",
      "58602\n",
      "58761\n",
      "58920\n",
      "已收集到 85 个帖子\n",
      "55969\n",
      "56489\n",
      "56668\n",
      "56847\n",
      "57029\n",
      "57168\n",
      "57987\n",
      "58106\n",
      "58225\n",
      "58384\n",
      "58503\n",
      "58602\n",
      "58761\n",
      "58920\n",
      "59039\n",
      "已收集到 85 个帖子\n",
      "55969\n",
      "56489\n",
      "56668\n",
      "56847\n",
      "57029\n",
      "57168\n",
      "57987\n",
      "58106\n",
      "58225\n",
      "58384\n",
      "58503\n",
      "58602\n",
      "58761\n",
      "58920\n",
      "59039\n",
      "59158\n",
      "已收集到 85 个帖子\n",
      "55969\n",
      "56489\n",
      "56668\n",
      "56847\n",
      "57029\n",
      "57168\n",
      "57987\n",
      "58106\n",
      "58225\n",
      "58384\n",
      "58503\n",
      "58602\n",
      "58761\n",
      "58920\n",
      "59039\n",
      "59158\n",
      "59257\n",
      "已收集到 85 个帖子\n",
      "55969\n",
      "56489\n",
      "56668\n",
      "56847\n",
      "57029\n",
      "57168\n",
      "57987\n",
      "58106\n",
      "58225\n",
      "58384\n",
      "58503\n",
      "58602\n",
      "58761\n",
      "58920\n",
      "59039\n",
      "59158\n",
      "59257\n",
      "59356\n",
      "已收集到 85 个帖子\n",
      "多次滚动后没有新数据，等60秒\n",
      "55969\n",
      "56489\n",
      "56668\n",
      "56847\n",
      "57029\n",
      "57168\n",
      "57987\n",
      "58106\n",
      "58225\n",
      "58384\n",
      "58503\n",
      "58602\n",
      "58761\n",
      "58920\n",
      "59039\n",
      "59158\n",
      "59257\n",
      "59356\n",
      "59534\n",
      "已收集到 85 个帖子\n",
      "多次滚动后没有新数据，已尝试 6\n",
      "停止爬取。\n",
      "爬取日期：2021-12-30~2022-12-30\n",
      "0\n",
      "0\n",
      "48\n",
      "158\n",
      "210\n",
      "219\n",
      "703\n",
      "1166\n",
      "1726\n",
      "2190\n",
      "已收集到 3 个帖子\n",
      "219\n",
      "703\n",
      "1166\n",
      "1726\n",
      "2190\n",
      "2859\n",
      "3548\n",
      "4211\n",
      "4894\n",
      "5563\n",
      "6252\n",
      "6961\n",
      "7650\n",
      "8273\n",
      "8644\n",
      "9287\n",
      "9731\n",
      "10195\n",
      "已收集到 16 个帖子\n",
      "4211\n",
      "4894\n",
      "5563\n",
      "6252\n",
      "6961\n",
      "7650\n",
      "8273\n",
      "8644\n",
      "9287\n",
      "9731\n",
      "10195\n",
      "10659\n",
      "11083\n",
      "11527\n",
      "12156\n",
      "12760\n",
      "13420\n",
      "13864\n",
      "14023\n",
      "14182\n",
      "14341\n",
      "14765\n",
      "已收集到 27 个帖子\n",
      "9287\n",
      "9731\n",
      "10195\n",
      "10659\n",
      "11083\n",
      "11527\n",
      "12156\n",
      "12760\n",
      "13420\n",
      "13864\n",
      "14023\n",
      "14182\n",
      "14341\n",
      "14765\n",
      "14964\n",
      "15773\n",
      "16632\n",
      "17277\n",
      "17847\n",
      "18670\n",
      "18829\n",
      "18988\n",
      "已收集到 35 个帖子\n",
      "13420\n",
      "13864\n",
      "14023\n",
      "14182\n",
      "14341\n",
      "14765\n",
      "14964\n",
      "15773\n",
      "16632\n",
      "17277\n",
      "17847\n",
      "18670\n",
      "18829\n",
      "18988\n",
      "19695\n",
      "20163\n",
      "20302\n",
      "21125\n",
      "21643\n",
      "22180\n",
      "22863\n",
      "23626\n",
      "已收集到 43 个帖子\n",
      "17847\n",
      "18670\n",
      "18829\n",
      "18988\n",
      "19695\n",
      "20163\n",
      "20302\n",
      "21125\n",
      "21643\n",
      "22180\n",
      "22863\n",
      "23626\n",
      "24275\n",
      "已收集到 43 个帖子\n",
      "22180\n",
      "22863\n",
      "23626\n",
      "24275\n",
      "24795\n",
      "25140\n",
      "已收集到 44 个帖子\n",
      "26642\n",
      "27348\n",
      "27909\n",
      "28612\n",
      "28914\n",
      "29351\n",
      "29871\n",
      "30554\n",
      "31048\n",
      "31230\n",
      "32053\n",
      "32696\n",
      "32835\n",
      "33561\n",
      "已收集到 57 个帖子\n",
      "31230\n",
      "32053\n",
      "32696\n",
      "32835\n",
      "33561\n",
      "34384\n",
      "34848\n",
      "34967\n",
      "已收集到 59 个帖子\n",
      "35949\n",
      "已收集到 59 个帖子\n",
      "40211\n",
      "41070\n",
      "41189\n",
      "41428\n",
      "41567\n",
      "42142\n",
      "42937\n",
      "43561\n",
      "43720\n",
      "已收集到 67 个帖子\n",
      "45238\n",
      "45833\n",
      "已收集到 68 个帖子\n",
      "49756\n",
      "49935\n",
      "50054\n",
      "50193\n",
      "50312\n",
      "50451\n",
      "50915\n",
      "51074\n",
      "51213\n",
      "已收集到 76 个帖子\n",
      "53983\n",
      "54686\n",
      "已收集到 77 个帖子\n",
      "58704\n",
      "58866\n",
      "已收集到 78 个帖子\n",
      "63024\n",
      "63349\n",
      "已收集到 79 个帖子\n",
      "67420\n",
      "已收集到 79 个帖子\n",
      "69253\n",
      "69773\n",
      "69995\n",
      "已收集到 81 个帖子\n",
      "69253\n",
      "69773\n",
      "69995\n",
      "70134\n",
      "已收集到 81 个帖子\n",
      "69253\n",
      "69773\n",
      "69995\n",
      "70134\n",
      "70463\n",
      "已收集到 81 个帖子\n",
      "69253\n",
      "69773\n",
      "69995\n",
      "70134\n",
      "70463\n",
      "70602\n",
      "70804\n",
      "已收集到 82 个帖子\n",
      "69253\n",
      "69773\n",
      "69995\n",
      "70134\n",
      "70463\n",
      "70602\n",
      "70804\n",
      "70923\n",
      "71122\n",
      "71241\n",
      "71423\n",
      "已收集到 85 个帖子\n",
      "69253\n",
      "69773\n",
      "69995\n",
      "70134\n",
      "70463\n",
      "70602\n",
      "70804\n",
      "70923\n",
      "71122\n",
      "71241\n",
      "71423\n",
      "72072\n",
      "已收集到 85 个帖子\n",
      "69253\n",
      "69773\n",
      "69995\n",
      "70134\n",
      "70463\n",
      "70602\n",
      "70804\n",
      "70923\n",
      "71122\n",
      "71241\n",
      "71423\n",
      "72072\n",
      "72191\n",
      "72658\n",
      "已收集到 86 个帖子\n",
      "69253\n",
      "69773\n",
      "69995\n",
      "70134\n",
      "70463\n",
      "70602\n",
      "70804\n",
      "70923\n",
      "71122\n",
      "71241\n",
      "71423\n",
      "72072\n",
      "72191\n",
      "72658\n",
      "72777\n",
      "已收集到 86 个帖子\n",
      "69253\n",
      "69773\n",
      "69995\n",
      "70134\n",
      "70463\n",
      "70602\n",
      "70804\n",
      "70923\n",
      "71122\n",
      "71241\n",
      "71423\n",
      "72072\n",
      "72191\n",
      "72658\n",
      "72777\n",
      "73036\n",
      "已收集到 86 个帖子\n",
      "69253\n",
      "69773\n",
      "69995\n",
      "70134\n",
      "70463\n",
      "70602\n",
      "70804\n",
      "70923\n",
      "71122\n",
      "71241\n",
      "71423\n",
      "72072\n",
      "72191\n",
      "72658\n",
      "72777\n",
      "73036\n",
      "73175\n",
      "已收集到 86 个帖子\n",
      "69253\n",
      "69773\n",
      "69995\n",
      "70134\n",
      "70463\n",
      "70602\n",
      "70804\n",
      "70923\n",
      "71122\n",
      "71241\n",
      "71423\n",
      "72072\n",
      "72191\n",
      "72658\n",
      "72777\n",
      "73036\n",
      "73175\n",
      "73334\n",
      "已收集到 86 个帖子\n",
      "69253\n",
      "69773\n",
      "69995\n",
      "70134\n",
      "70463\n",
      "70602\n",
      "70804\n",
      "70923\n",
      "71122\n",
      "71241\n",
      "71423\n",
      "72072\n",
      "72191\n",
      "72658\n",
      "72777\n",
      "73036\n",
      "73175\n",
      "73334\n",
      "73533\n",
      "已收集到 86 个帖子\n",
      "多次滚动后没有新数据，等60秒\n",
      "69253\n",
      "69773\n",
      "69995\n",
      "70134\n",
      "70463\n",
      "70602\n",
      "70804\n",
      "70923\n",
      "71122\n",
      "71241\n",
      "71423\n",
      "72072\n",
      "72191\n",
      "72658\n",
      "72777\n",
      "73036\n",
      "73175\n",
      "73334\n",
      "73533\n",
      "73692\n",
      "73811\n",
      "已收集到 87 个帖子\n",
      "69253\n",
      "69773\n",
      "69995\n",
      "70134\n",
      "70463\n",
      "70602\n",
      "70804\n",
      "70923\n",
      "71122\n",
      "71241\n",
      "71423\n",
      "72072\n",
      "72191\n",
      "72658\n",
      "72777\n",
      "73036\n",
      "73175\n",
      "73334\n",
      "73533\n",
      "73692\n",
      "73811\n",
      "已收集到 87 个帖子\n",
      "69253\n",
      "69773\n",
      "69995\n",
      "70134\n",
      "70463\n",
      "70602\n",
      "70804\n",
      "70923\n",
      "71122\n",
      "71241\n",
      "71423\n",
      "72072\n",
      "72191\n",
      "72658\n",
      "72777\n",
      "73036\n",
      "73175\n",
      "73334\n",
      "73533\n",
      "73692\n",
      "73811\n",
      "已收集到 87 个帖子\n",
      "69253\n",
      "69773\n",
      "69995\n",
      "70134\n",
      "70463\n",
      "70602\n",
      "70804\n",
      "70923\n",
      "71122\n",
      "71241\n",
      "71423\n",
      "72072\n",
      "72191\n",
      "72658\n",
      "72777\n",
      "73036\n",
      "73175\n",
      "73334\n",
      "73533\n",
      "73692\n",
      "73811\n",
      "已收集到 87 个帖子\n",
      "69253\n",
      "69773\n",
      "69995\n",
      "70134\n",
      "70463\n",
      "70602\n",
      "70804\n",
      "70923\n",
      "71122\n",
      "71241\n",
      "71423\n",
      "72072\n",
      "72191\n",
      "72658\n",
      "72777\n",
      "73036\n",
      "73175\n",
      "73334\n",
      "73533\n",
      "73692\n",
      "73811\n",
      "已收集到 87 个帖子\n",
      "69253\n",
      "69773\n",
      "69995\n",
      "70134\n",
      "70463\n",
      "70602\n",
      "70804\n",
      "70923\n",
      "71122\n",
      "71241\n",
      "71423\n",
      "72072\n",
      "72191\n",
      "72658\n",
      "72777\n",
      "73036\n",
      "73175\n",
      "73334\n",
      "73533\n",
      "73692\n",
      "73811\n",
      "已收集到 87 个帖子\n",
      "多次滚动后没有新数据，等60秒\n",
      "69253\n",
      "69773\n",
      "69995\n",
      "70134\n",
      "70463\n",
      "70602\n",
      "70804\n",
      "70923\n",
      "71122\n",
      "71241\n",
      "71423\n",
      "72072\n",
      "72191\n",
      "72658\n",
      "72777\n",
      "73036\n",
      "73175\n",
      "73334\n",
      "73533\n",
      "73692\n",
      "73811\n",
      "已收集到 87 个帖子\n",
      "多次滚动后没有新数据，已尝试 6\n",
      "停止爬取。\n",
      "爬取日期：2022-12-30~2023-12-30\n",
      "0\n",
      "0\n",
      "48\n",
      "158\n",
      "210\n",
      "219\n",
      "683\n",
      "1287\n",
      "2004\n",
      "2488\n",
      "3237\n",
      "3946\n",
      "4430\n",
      "已收集到 6 个帖子\n",
      "219\n",
      "683\n",
      "1287\n",
      "2004\n",
      "2488\n",
      "3237\n",
      "3946\n",
      "4430\n",
      "5179\n",
      "5683\n",
      "6434\n",
      "7123\n",
      "7772\n",
      "8407\n",
      "8971\n",
      "9620\n",
      "10303\n",
      "已收集到 14 个帖子\n",
      "4430\n",
      "5179\n",
      "5683\n",
      "6434\n",
      "7123\n",
      "7772\n",
      "8407\n",
      "8971\n",
      "9620\n",
      "10303\n",
      "11032\n",
      "11496\n",
      "12245\n",
      "12809\n",
      "13612\n",
      "14116\n",
      "已收集到 20 个帖子\n",
      "8971\n",
      "9620\n",
      "10303\n",
      "11032\n",
      "11496\n",
      "12245\n",
      "12809\n",
      "13612\n",
      "14116\n",
      "14885\n",
      "15124\n",
      "15548\n",
      "16351\n",
      "16917\n",
      "17566\n",
      "18249\n",
      "18611\n",
      "19360\n",
      "已收集到 29 个帖子\n",
      "13612\n",
      "14116\n",
      "14885\n",
      "15124\n",
      "15548\n",
      "16351\n",
      "16917\n",
      "17566\n",
      "18249\n",
      "18611\n",
      "19360\n",
      "19959\n",
      "20630\n",
      "21114\n",
      "21600\n",
      "22260\n",
      "22983\n",
      "23523\n",
      "已收集到 36 个帖子\n",
      "18249\n",
      "18611\n",
      "19360\n",
      "19959\n",
      "20630\n",
      "21114\n",
      "21600\n",
      "22260\n",
      "22983\n",
      "23523\n",
      "23947\n",
      "24482\n",
      "24721\n",
      "25438\n",
      "26141\n",
      "26758\n",
      "27418\n",
      "27577\n",
      "28141\n",
      "已收集到 45 个帖子\n",
      "22260\n",
      "22983\n",
      "23523\n",
      "23947\n",
      "24482\n",
      "24721\n",
      "25438\n",
      "26141\n",
      "26758\n",
      "27418\n",
      "27577\n",
      "28141\n",
      "28924\n",
      "29414\n",
      "29553\n",
      "30037\n",
      "30216\n",
      "30817\n",
      "31540\n",
      "31699\n",
      "32482\n",
      "已收集到 54 个帖子\n",
      "26758\n",
      "27418\n",
      "27577\n",
      "28141\n",
      "28924\n",
      "29414\n",
      "29553\n",
      "30037\n",
      "30216\n",
      "30817\n",
      "31540\n",
      "31699\n",
      "32482\n",
      "33211\n",
      "33854\n",
      "34537\n",
      "35255\n",
      "35699\n",
      "36103\n",
      "36663\n",
      "已收集到 60 个帖子\n",
      "31699\n",
      "32482\n",
      "33211\n",
      "33854\n",
      "34537\n",
      "35255\n",
      "35699\n",
      "36103\n",
      "36663\n",
      "36901\n",
      "已收集到 60 个帖子\n",
      "36103\n",
      "36663\n",
      "36901\n",
      "37630\n",
      "37729\n",
      "38153\n",
      "已收集到 62 个帖子\n",
      "40603\n",
      "41007\n",
      "41996\n",
      "42420\n",
      "42709\n",
      "42888\n",
      "43312\n",
      "43494\n",
      "43958\n",
      "44674\n",
      "已收集到 71 个帖子\n",
      "45257\n",
      "已收集到 71 个帖子\n",
      "49488\n",
      "49992\n",
      "已收集到 72 个帖子\n",
      "54166\n",
      "54325\n",
      "54444\n",
      "54623\n",
      "已收集到 75 个帖子\n",
      "58318\n",
      "已收集到 75 个帖子\n",
      "63052\n",
      "已收集到 75 个帖子\n",
      "67408\n",
      "68137\n",
      "68256\n",
      "已收集到 77 个帖子\n",
      "71746\n",
      "已收集到 77 个帖子\n",
      "76635\n",
      "76817\n",
      "77317\n",
      "77479\n",
      "已收集到 80 个帖子\n",
      "77479\n",
      "78219\n",
      "已收集到 80 个帖子\n",
      "77479\n",
      "78219\n",
      "78404\n",
      "已收集到 80 个帖子\n",
      "77479\n",
      "78219\n",
      "78404\n",
      "78523\n",
      "78642\n",
      "已收集到 81 个帖子\n",
      "77479\n",
      "78219\n",
      "78404\n",
      "78523\n",
      "78642\n",
      "79421\n",
      "已收集到 81 个帖子\n",
      "77479\n",
      "78219\n",
      "78404\n",
      "78523\n",
      "78642\n",
      "79421\n",
      "79865\n",
      "已收集到 81 个帖子\n",
      "77479\n",
      "78219\n",
      "78404\n",
      "78523\n",
      "78642\n",
      "79421\n",
      "79865\n",
      "79984\n",
      "已收集到 81 个帖子\n",
      "77479\n",
      "78219\n",
      "78404\n",
      "78523\n",
      "78642\n",
      "79421\n",
      "79865\n",
      "79984\n",
      "80103\n",
      "已收集到 81 个帖子\n",
      "77479\n",
      "78219\n",
      "78404\n",
      "78523\n",
      "78642\n",
      "79421\n",
      "79865\n",
      "79984\n",
      "80103\n",
      "80242\n",
      "已收集到 81 个帖子\n",
      "多次滚动后没有新数据，等60秒\n",
      "77479\n",
      "78219\n",
      "78404\n",
      "78523\n",
      "78642\n",
      "79421\n",
      "79865\n",
      "79984\n",
      "80103\n",
      "80242\n",
      "80341\n",
      "已收集到 81 个帖子\n",
      "多次滚动后没有新数据，已尝试 6\n",
      "停止爬取。\n"
     ]
    }
   ],
   "source": [
    "# get_posts_day_by_day('genetically modified organism', '2014-01-01', '2023-01-01', False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88eadf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_posts_day_by_day('childless cat', '2024-09-11', '2024-09-17', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "951f2cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_posts_day_by_day('childless cat', '2024-09-11', '2024-09-17', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eff895af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_posts_day_by_day('childless cat', '2024-09-15', '2024-09-17', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f8a43ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标题: X\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import random\n",
    "import json\n",
    "\n",
    "#先运行命令再运行此代码块 & \"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\" --remote-debugging-port=9530 --user-data-dir=\"D:\\selenium\\AutomationProfile\"\n",
    "chromedriver_path = \"C:/Program Files/Google/Chrome/Application/chromedriver.exe\"\n",
    "\n",
    "option = Options()\n",
    "option.add_experimental_option(\"debuggerAddress\", \"127.0.0.1:9530\")\n",
    "\n",
    "# 创建 ChromeDriver 服务\n",
    "service = Service(chromedriver_path)\n",
    "\n",
    "# 初始化 WebDriver\n",
    "driver = webdriver.Chrome(service=service, options=option)\n",
    "\n",
    "# 去除 WebDriver 痕迹\n",
    "driver.execute_cdp_cmd(\"Page.addScriptToEvaluateOnNewDocument\", {\n",
    "    \"source\": \"\"\"\n",
    "    Object.defineProperty(navigator, 'webdriver', {\n",
    "      get: () => undefined\n",
    "    });\n",
    "    \"\"\"\n",
    "})\n",
    "\n",
    "# 设置隐式等待\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# 测试访问页面\n",
    "driver.get(\"https://www.x.com\")\n",
    "print(\"标题:\", driver.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1625f8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver import ActionChains\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "def get_post_comments(main_user_id, main_post_id):\n",
    "    posts_lists = []\n",
    "    post_index = set() #帖子独立标识集合去重\n",
    "    # 打开目标帖子的URL\n",
    "    post_url = f'https://x.com/{main_user_id}/status/{main_post_id}'\n",
    "    driver.get(post_url)\n",
    "    time.sleep(10)  # 等待页面加载\n",
    "\n",
    "    comments = []  # 存储评论的列表\n",
    "\n",
    "    # 滚动页面以加载更多评论（如果需要）\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    scroll_step = 2000  # 每次向下滚动的像素数\n",
    "    scroll_count = 0\n",
    "    previous_post_count = 0  # 用于记录上一次的帖子ID数量\n",
    "    max_attempts = 3  # 当数据条数不变的最大允许次数\n",
    "    attempts = 0\n",
    "    garbages = []\n",
    "    isgarbage = 0\n",
    "    parent_comment_id = None\n",
    "    # 保存路径\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "    base_dir = f\"twitter_comments_new\"\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "    posts_file = os.path.join(base_dir, f\"[top rank]comments_genetically modified organism_{main_post_id}.json\") # 为了我的转基因case特制的文件名\n",
    "    \n",
    "    while True:\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'lxml')\n",
    "        post_tags = soup.select('div[data-testid=\"cellInnerDiv\"]')\n",
    "        for post_tag in post_tags:\n",
    "            transform_value = post_tag['style'].split('translateY(')[1].split('px)')[0]\n",
    "            if float(transform_value)>0 and transform_value not in post_index: # todo：做一下真的采集到手后再去重的逻辑\n",
    "                # post_index.add(transform_value)\n",
    "                # print(transform_value)\n",
    "\n",
    "                # 提取帖子发布时间\n",
    "                time_tag = post_tag.find('time', attrs={'datetime': True})\n",
    "                if time_tag and 'datetime' in time_tag.attrs:\n",
    "                    datetime_str = time_tag['datetime']\n",
    "                    # 解析ISO格式的日期时间字符串\n",
    "                    dt = datetime.strptime(datetime_str, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "                    # 格式化为“年-月-日 时:分:秒”格式\n",
    "                    post_time = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                else:\n",
    "                    post_time = ''\n",
    "                if not post_time and post_tag.find('span', class_='css-1jxf684 r-bcqeeo r-1ttztb7 r-qvutc0 r-poiln3'):\n",
    "                    info_tag = post_tag.find('span', class_='css-1jxf684 r-bcqeeo r-1ttztb7 r-qvutc0 r-poiln3')\n",
    "                    info = info_tag.select_one('span').text.strip() if info_tag.select_one('span') else ''\n",
    "                    if info == 'This Post is unavailable.':\n",
    "                        parent_comment_id = None\n",
    "                        continue\n",
    "                \n",
    "                \n",
    "                # 提取用户名\n",
    "                username_tag = post_tag.select_one('div[data-testid=\"User-Name\"]')\n",
    "                if username_tag:\n",
    "                    username = username_tag.get_text(strip=True)\n",
    "                    # 检查用户名是否包含垃圾词\n",
    "                    if any(garbage in username for garbage in garbages):\n",
    "                        isgarbage = 1\n",
    "                        continue  # 跳过当前帖子，不再提取ID\n",
    "                    username = username_tag.select_one('span').text.strip() if username_tag.select_one('span') else ''\n",
    "                    # print(username)\n",
    "                else:\n",
    "                    # print(\"no username\")\n",
    "                    parent_comment_id = None\n",
    "                    continue\n",
    "                # # 提取用户名\n",
    "                # username_span = post_tag.find('span', class_='css-1jxf684 r-bcqeeo r-1ttztb7 r-qvutc0 r-poiln3')\n",
    "                # if username_span:\n",
    "                #     username = username_span.get_text()\n",
    "                # else:\n",
    "                #     username = 'Unknown'\n",
    "                # print(username)\n",
    "                isgarbage = 0\n",
    "                \n",
    "\n",
    "                # 提取帖子正文\n",
    "                # post_content = post_tag.select_one('div[data-testid=\"tweetText\"]').text.strip() if post_tag.select_one('div[data-testid=\"tweetText\"]') else ''\n",
    "            # 提取帖子正文，并保持图片和文字顺序\n",
    "                post_content = \"\"\n",
    "                tweet_text_div = post_tag.select_one('div[data-testid=\"tweetText\"]')\n",
    "                if tweet_text_div:\n",
    "                    # 遍历所有子元素，按顺序提取文字、图片和链接\n",
    "                    for element in tweet_text_div.children:\n",
    "                        if element.name == 'span':  # 处理文本\n",
    "                            post_content += element.text.strip() + \" \"\n",
    "                        elif element.name == 'img':  # 处理图片\n",
    "                            alt_text = element.get('alt', '')\n",
    "                            if alt_text:\n",
    "                                post_content += f\"{alt_text} \"\n",
    "                        elif element.name == 'a':  # 处理链接\n",
    "                            href = element.get('href', '')\n",
    "                            if href:\n",
    "                                post_content += f\"{href} \"\n",
    "                                post_content += element.text.strip() + \" \"\n",
    "                # print(post_content)\n",
    "\n",
    "                # 查找包含aria-label的div标签，提取转评赞以及帖子id\n",
    "                cnt_tag = None\n",
    "                if post_time < '2023-01-01':\n",
    "                    # print(post_time)\n",
    "                    aria_label = post_tag.find('div', attrs={'aria-label': re.compile(r'.*Likes.*')}) # likes views\n",
    "                    cnt_tag = post_tag.find_all('button', class_='css-175oi2r r-1777fci r-bt1l66 r-bztko3 r-lrvibr r-1loqt21 r-1ny4l3l')\n",
    "                else:\n",
    "                    aria_label = post_tag.find('div', attrs={'aria-label': re.compile(r'.*views.*')}) # likes views\n",
    "\n",
    "                if cnt_tag and post_time < '2023-01-01':\n",
    "                    # print(\"has cnt_tag\")\n",
    "                    # 从aria-label中提取数据\n",
    "                    label_text = cnt_tag[3]['aria-label']\n",
    "\n",
    "                    # 初始化数据\n",
    "                    replies = reposts = likes = bookmarks = views = 0\n",
    "\n",
    "                    # 使用正则表达式逐项匹配\n",
    "                    # if 'replie' in label_text:\n",
    "                    #     replies = int(re.search(r'(\\d+)\\s+replie', label_text).group(1))\n",
    "                    # if 'repost' in label_text:\n",
    "                    #     reposts = int(re.search(r'(\\d+)\\s+repost', label_text).group(1))\n",
    "                    # if 'like' in label_text:\n",
    "                    #     likes = int(re.search(r'(\\d+)\\s+like', label_text).group(1))\n",
    "                    # if 'bookmark' in label_text:\n",
    "                    #     bookmarks = int(re.search(r'(\\d+)\\s+bookmark', label_text).group(1))\n",
    "                    # 修改代码，处理None的情况\n",
    "                    for t in cnt_tag:\n",
    "                        if 'Like' in t['aria-label']:\n",
    "                            label_text = t['aria-label']\n",
    "                    if 'Like' in label_text:\n",
    "                        match = re.search(r'(\\d+)\\s+Like', label_text)\n",
    "                        likes = int(match.group(1)) if match else 0\n",
    "                    # print(likes)\n",
    "                    for t in cnt_tag:\n",
    "                        if 'Repl' in t['aria-label']:\n",
    "                            label_text = t['aria-label']\n",
    "                    if 'Repl' in label_text:\n",
    "                        match = re.search(r'(\\d+)\\s+Repl', label_text)\n",
    "                        replies = int(match.group(1)) if match else 0\n",
    "                    for t in cnt_tag:\n",
    "                        if 'repost' in t['aria-label']:\n",
    "                            label_text = t['aria-label']\n",
    "                    if 'repost' in label_text:\n",
    "                        match = re.search(r'(\\d+)\\s+repost', label_text)\n",
    "                        reposts = int(match.group(1)) if match else 0\n",
    "                    views = 0\n",
    "\n",
    "                    # 查找帖子ID和用户ID\n",
    "                    post_id_tag = post_tag.find_all('div', class_='css-175oi2r r-18u37iz r-1h0z5md r-13awgt0')\n",
    "                    if post_id_tag:\n",
    "                        # 找到第四个div，提取其中的链接\n",
    "                        post_id_link = post_id_tag[3].find('a', href=True)\n",
    "                        if post_id_link:\n",
    "                            post_id = post_id_link['href'].split('/')[3]  # 从URL中提取帖子ID\n",
    "                            user_id = post_id_link['href'].split('/')[1]\n",
    "                        else:\n",
    "                            post_id = ''\n",
    "                            user_id = ''\n",
    "                    else:\n",
    "                        post_id = ''\n",
    "                        user_id = ''\n",
    "                    # print(post_id)\n",
    "\n",
    "                elif aria_label and post_time >= '2023-01-01':\n",
    "                    # print(\"has aria_label\")\n",
    "                    # 从aria-label中提取数据\n",
    "                    label_text = aria_label['aria-label']\n",
    "\n",
    "                    # 初始化数据\n",
    "                    replies = reposts = likes = bookmarks = views = 0\n",
    "\n",
    "                    # 使用正则表达式逐项匹配\n",
    "                    # if 'replie' in label_text:\n",
    "                    #     replies = int(re.search(r'(\\d+)\\s+replie', label_text).group(1))\n",
    "                    # if 'repost' in label_text:\n",
    "                    #     reposts = int(re.search(r'(\\d+)\\s+repost', label_text).group(1))\n",
    "                    # if 'like' in label_text:\n",
    "                    #     likes = int(re.search(r'(\\d+)\\s+like', label_text).group(1))\n",
    "                    # if 'bookmark' in label_text:\n",
    "                    #     bookmarks = int(re.search(r'(\\d+)\\s+bookmark', label_text).group(1))\n",
    "                    # 修改代码，处理None的情况\n",
    "                    if 'repl' in label_text:\n",
    "                        match = re.search(r'(\\d+)\\s+repl', label_text)\n",
    "                        replies = int(match.group(1)) if match else 0\n",
    "\n",
    "                    if 'repost' in label_text:\n",
    "                        match = re.search(r'(\\d+)\\s+repost', label_text)\n",
    "                        reposts = int(match.group(1)) if match else 0\n",
    "\n",
    "                    if 'like' in label_text:\n",
    "                        match = re.search(r'(\\d+)\\s+like', label_text)\n",
    "                        likes = int(match.group(1)) if match else 0\n",
    "\n",
    "                    if 'bookmark' in label_text:\n",
    "                        match = re.search(r'(\\d+)\\s+bookmark', label_text)\n",
    "                        bookmarks = int(match.group(1)) if match else 0\n",
    "                    if 'view' in label_text:\n",
    "                        # views = int(re.search(r'(\\d+)\\s+view', label_text).group(1))\n",
    "                        # 检查是否匹配到了 \"view\" 和数字\n",
    "                        match = re.search(r'(\\d+)\\s+view', label_text)\n",
    "                        if match:\n",
    "                            views = int(match.group(1))\n",
    "                        else:\n",
    "                            views = 0  # 如果没有匹配到，设置默认值为0\n",
    "\n",
    "                    # 查找帖子ID和用户ID\n",
    "                    post_id_tag = aria_label.find_all('div', class_='css-175oi2r r-18u37iz r-1h0z5md r-13awgt0')\n",
    "                    if post_id_tag:\n",
    "                        # 找到第四个div，提取其中的链接\n",
    "                        post_id_link = post_id_tag[3].find('a', href=True)\n",
    "                        if post_id_link:\n",
    "                            post_id = post_id_link['href'].split('/')[3]  # 从URL中提取帖子ID\n",
    "                            user_id = post_id_link['href'].split('/')[1]\n",
    "                        else:\n",
    "                            post_id = ''\n",
    "                            user_id = ''\n",
    "                    else:\n",
    "                        post_id = ''\n",
    "                        user_id = ''\n",
    "                else:\n",
    "                    replies = reposts = likes = bookmarks = views = 0\n",
    "                    post_id = ''\n",
    "                    user_id = ''\n",
    "                    parent_comment_id = None\n",
    "                    break\n",
    "\n",
    "                # 提取hashtags，确保唯一性，使用集合去重\n",
    "                hashtags = set()\n",
    "                hashtag_tags = post_tag.select('a[href*=\"/hashtag/\"]')\n",
    "                for tag in hashtag_tags:\n",
    "                    hashtag = tag.get_text(strip=True)\n",
    "                    hashtags.add(hashtag)\n",
    "\n",
    "                # 提取图片和视频的链接\n",
    "                media_urls = set()  # 使用集合来自动去重\n",
    "                img_tags = post_tag.find_all('img')\n",
    "                for img in img_tags:\n",
    "                    img_url = img.get('src')\n",
    "                    if img_url and 'profile_images' not in img_url and 'emoji' not in img_url:\n",
    "                        media_urls.add(img_url)\n",
    "                media_urls = list(media_urls)\n",
    "\n",
    "                # 提取@了哪些用户\n",
    "                at_usernames = set()  # 使用集合确保唯一性\n",
    "                at_usernames.update(re.findall(r'@(\\w+)', post_content))  # 查找所有 @ 后的用户名\n",
    "\n",
    "                # 提取帖子中的所有链接（使用正则表达式从文本中匹配URL）\n",
    "                urls = set()  # 使用集合去重\n",
    "\n",
    "                # 匹配以 http 或 https 开头的 URL\n",
    "                url_pattern = re.compile(r'https?://[^\\s]+')\n",
    "\n",
    "                # 在帖子文本中查找所有符合条件的URL\n",
    "                urls.update(url_pattern.findall(post_content))\n",
    "\n",
    "                # 查找回复的多个用户\n",
    "                reply_users = []\n",
    "                reply_tag = post_tag.find('div', class_='css-146c3p1 r-bcqeeo r-1ttztb7 r-qvutc0 r-37j5jr r-a023e6 r-rjixqe r-16dba41') # 查找包含回复信息的 div\n",
    "                if reply_tag:\n",
    "                    # 获取所有回复用户的 a 标签\n",
    "                    reply_user_tags = reply_tag.find_all('a')\n",
    "                    for reply_user_tag in reply_user_tags:\n",
    "                        # 提取用户名并添加到列表\n",
    "                        reply_users.append(reply_user_tag.text.strip())\n",
    "\n",
    "                # 将评论及用户信息存储在字典中\n",
    "                comment_data = {\n",
    "                        \"postUrl\": f\"https://x.com/{user_id}/status/{post_id}\",\n",
    "                        \"mid\": post_id,\n",
    "                        \"text\": post_content,\n",
    "                        \"date\": post_time,\n",
    "                        \"userId\": user_id,\n",
    "                        \"userName\": username,\n",
    "                        \"source_post_id\": main_post_id,  # 主帖ID\n",
    "                        \"parent_comment_id\": parent_comment_id,  # 父级ID\n",
    "                        \"likeNum\": likes,\n",
    "                        \"commentNum\": replies,\n",
    "                        \"repostNum\": reposts,\n",
    "                        \"viewNum\": views,\n",
    "                        \"hashtags\": list(hashtags),\n",
    "                        \"mediaUrls\": media_urls,\n",
    "                        \"atUsernames\": list(at_usernames),  # 新增字段：记录@的用户\n",
    "                        \"urls\": list(urls),  # 新增字段：记录帖中的所有链接\n",
    "                        \"replyUsers\": reply_users  # 新增字段：记录多个回复的用户\n",
    "                }\n",
    "                posts_lists.append(comment_data)\n",
    "                post_index.add(transform_value)\n",
    "\n",
    "                # 保存 posts\n",
    "                with open(posts_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(list(posts_lists), f, ensure_ascii=False, indent=4)\n",
    "\n",
    "                reply_line = post_tag.find('div', class_='css-175oi2r r-1bnu78o r-f8sm7e r-m5arl1 r-16y2uox r-14gqq1x')\n",
    "                if reply_line:\n",
    "                    parent_comment_id = post_id\n",
    "                else:\n",
    "                    parent_comment_id = None\n",
    "\n",
    "        current_post_count = len(posts_lists)\n",
    "        print(f\"已收集到 {current_post_count} 个评论帖\")\n",
    "\n",
    "        if current_post_count >= 3000:  # 如果爬取到的评论条数超过3000，则停止爬取\n",
    "             break\n",
    "\n",
    "        if current_post_count == previous_post_count and not isgarbage: #防止遇到批量垃圾帖子，导致误识别为滑到底了\n",
    "            time.sleep(random.uniform(1, 2))\n",
    "            attempts += 1\n",
    "        else:\n",
    "            attempts = 0\n",
    "            previous_post_count = current_post_count\n",
    "\n",
    "        if attempts == max_attempts:\n",
    "            # 检测到页面错误信息，尝试点击'Retry'按钮...\n",
    "            if len(post_tags) > 0 and post_tags[-1].select_one('span').text.strip() == \"Something went wrong. Try reloading.\" if post_tags[-1].select_one('span') else '':\n",
    "                print(f\"检测到'Retry'按钮，当前时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')},滚动了{scroll_count}次\")\n",
    "                # 等待人工确认点击\n",
    "                # input(\"按Enter键以继续爬取...\")\n",
    "                while True:  # 无限循环，直到成功点击\n",
    "                    try:\n",
    "                        # 等待6分钟（360秒）\n",
    "                        print(\"等待6分钟...\")\n",
    "                        time.sleep(360)  # 等待6分钟\n",
    "\n",
    "                        # 使用 CSS Selector 或 XPath 查找'Retry'按钮并确保它可点击\n",
    "                        retrybutton = WebDriverWait(driver, 3).until(\n",
    "                            EC.element_to_be_clickable((By.XPATH, \"//button[.//*[contains(text(),'Retry')]]\"))\n",
    "                        )\n",
    "\n",
    "                        # 滚动到按钮所在位置\n",
    "                        driver.execute_script(\"arguments[0].scrollIntoView();\", retrybutton)\n",
    "\n",
    "                        # 强制点击按钮\n",
    "                        driver.execute_script(\"arguments[0].click();\", retrybutton)\n",
    "                        print(f\"点击'Retry'按钮成功，等待5秒后继续爬取，当前时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "                        time.sleep(15)  # 等待15秒后继续爬取\n",
    "                        attempts = 0\n",
    "                        previous_post_count = current_post_count\n",
    "                        break  # 成功点击后跳出循环\n",
    "\n",
    "                    except Exception as e:\n",
    "                        # print(f\"未能找到'Retry'按钮或点击失败，错误信息: {e}\")\n",
    "                        print(\"没找到按钮\")\n",
    "\n",
    "            print(\"多次滚动后没有新数据，等10秒\")\n",
    "            time.sleep(10)\n",
    "\n",
    "        if attempts > max_attempts:  # 如果连续6次没有新数据，停止爬取\n",
    "            print(f\"多次滚动后没有新数据，已尝试 {attempts}\")\n",
    "            break\n",
    "            # user_input = input(\"请输入0停止爬取，1继续爬取: \")  # 获取用户输入\n",
    "            # if user_input == \"0\":\n",
    "            #     print(\"停止爬取。\")\n",
    "            #     break  # 停止爬取\n",
    "            # elif user_input == \"1\":\n",
    "            #     print(\"继续爬取...\")\n",
    "            #     attempts = 0  # 重置尝试次数，继续爬取\n",
    "            # else:\n",
    "            #     print(\"无效输入，请输入0或1。\")\n",
    "\n",
    "        driver.execute_script(f\"window.scrollBy(0, {scroll_step});\")\n",
    "        scroll_count += 1\n",
    "        time.sleep(random.uniform(2, 3))  # 等待加载更多内容\n",
    "\n",
    "# 测试爬取一个帖子的评论\n",
    "# get_post_comments('kohkalah', '1804259338766241930')\n",
    "# get_post_comments('RFKJrHealthSec', '1867589034274340942')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "636a3229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "966\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder_path = 'twitter_posts_new'  # 替换为你的文件夹路径\n",
    "posturls_file = '[top rank]posturls_genetically modified organism_2014_2025.txt'\n",
    "\n",
    "# 用于存储读取的postUrls\n",
    "post_urls = []\n",
    "\n",
    "# 读取文件中的每一行，去掉换行符并存储到列表中\n",
    "with open(os.path.join(folder_path, posturls_file), 'r', encoding='utf-8') as file:\n",
    "    post_urls = [line.strip() for line in file.readlines()]\n",
    "print(len(post_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1f48b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNAM_MX\n",
      "995121432135905280\n"
     ]
    }
   ],
   "source": [
    "test_url = 'https://x.com/UNAM_MX/status/995121432135905280'\n",
    "print(test_url.split('/')[3])\n",
    "print(test_url.split('/')[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4e585b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://x.com/Nature/status/718467367106383872\n"
     ]
    }
   ],
   "source": [
    "#从ptr_url.txt中读取ptr_url\n",
    "with open(os.path.join(folder_path,'ptr_url.txt'), 'r') as file:\n",
    "    ptr_url = [line.strip() for line in file.readlines()][0]\n",
    "print(ptr_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba74a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesleep_cnt = 0\n",
    "after_ptr_url = False\n",
    "for post_url in post_urls:\n",
    "    if post_url == ptr_url:\n",
    "        print(\"到达指针位置\")\n",
    "        after_ptr_url = True\n",
    "    if after_ptr_url:\n",
    "        user_id = post_url.split('/')[3]\n",
    "        post_id = post_url.split('/')[5]\n",
    "        print(f\"正在爬取帖子评论：{user_id}/{post_id}\")\n",
    "        ptr_url = post_url\n",
    "        # 存储ptr_url到文件\n",
    "        with open(os.path.join(folder_path, 'ptr_url.txt'), 'w', encoding='utf-8') as file:\n",
    "            file.write(ptr_url)\n",
    "        get_post_comments(user_id, post_id)\n",
    "        timesleep_cnt += 1\n",
    "        if timesleep_cnt % 20 == 0:\n",
    "            print(f\"已爬取{timesleep_cnt}个主帖的评论，休息5分钟...\")\n",
    "            time.sleep(300)\n",
    "        else:\n",
    "            time.sleep(5)\n",
    "    else:\n",
    "        print(f\"{post_url}已被采集过评论\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "85822d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标题: X\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import random\n",
    "import json\n",
    "\n",
    "#先运行命令再运行此代码块 & \"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\" --remote-debugging-port=9530 --user-data-dir=\"D:\\selenium\\AutomationProfile\"\n",
    "chromedriver_path = \"C:/Program Files/Google/Chrome/Application/chromedriver.exe\"\n",
    "\n",
    "option = Options()\n",
    "option.add_experimental_option(\"debuggerAddress\", \"127.0.0.1:9530\")\n",
    "\n",
    "# 创建 ChromeDriver 服务\n",
    "service = Service(chromedriver_path)\n",
    "\n",
    "# 初始化 WebDriver\n",
    "driver = webdriver.Chrome(service=service, options=option)\n",
    "\n",
    "# 去除 WebDriver 痕迹\n",
    "driver.execute_cdp_cmd(\"Page.addScriptToEvaluateOnNewDocument\", {\n",
    "    \"source\": \"\"\"\n",
    "    Object.defineProperty(navigator, 'webdriver', {\n",
    "      get: () => undefined\n",
    "    });\n",
    "    \"\"\"\n",
    "})\n",
    "\n",
    "# 设置隐式等待\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# 测试访问页面\n",
    "driver.get(\"https://www.x.com\")\n",
    "print(\"标题:\", driver.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0d1d0d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver import ActionChains\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "def get_post_quotes(main_user_id, main_post_id):\n",
    "    posts_lists = []\n",
    "    post_index = set() #帖子独立标识集合去重\n",
    "    # 打开目标帖子的URL\n",
    "    post_url = f'https://x.com/{main_user_id}/status/{main_post_id}/quotes'\n",
    "    driver.get(post_url)\n",
    "    time.sleep(10)  # 等待页面加载\n",
    "\n",
    "    quotes = []  # 存储评论的列表\n",
    "\n",
    "    # 滚动页面以加载更多评论（如果需要）\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    scroll_step = 4500  # 每次向下滚动的像素数\n",
    "    scroll_count = 0\n",
    "    previous_post_count = 0  # 用于记录上一次的帖子ID数量\n",
    "    max_attempts = 3  # 当数据条数不变的最大允许次数\n",
    "    attempts = 0\n",
    "    garbages = []\n",
    "    isgarbage = 0\n",
    "    # 保存路径\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "    base_dir = f\"twitter_quotes\"\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "    posts_file = os.path.join(base_dir, f\"[top rank]quotes_genetically modified organism_{main_post_id}.json\") # 为了我的转基因case特制的文件名\n",
    "    \n",
    "    while True:\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'lxml')\n",
    "        empty_tag = soup.select('div[data-testid=\"empty_state_header_text\"]')\n",
    "        if empty_tag:\n",
    "            print(\"no quotes\")\n",
    "            break\n",
    "        post_tags = soup.select('div[data-testid=\"cellInnerDiv\"]')\n",
    "        for post_tag in post_tags:\n",
    "            transform_value = post_tag['style'].split('translateY(')[1].split('px)')[0]\n",
    "            if transform_value not in post_index: # todo：做一下真的采集到手后再去重的逻辑\n",
    "                # post_index.add(transform_value)\n",
    "                # print(transform_value)\n",
    "\n",
    "                info_tag = post_tag.find('div', class_='css-175oi2r r-k4xj1c r-18u37iz r-1wtj0ep')\n",
    "                if not info_tag:\n",
    "                    continue\n",
    "                # 提取帖子发布时间\n",
    "                time_tag = info_tag.find('time', attrs={'datetime': True})\n",
    "                if time_tag and 'datetime' in time_tag.attrs:\n",
    "                    datetime_str = time_tag['datetime']\n",
    "                    # 解析ISO格式的日期时间字符串\n",
    "                    dt = datetime.strptime(datetime_str, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "                    # 格式化为“年-月-日 时:分:秒”格式\n",
    "                    post_time = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                else:\n",
    "                    post_time = ''\n",
    "                \n",
    "                \n",
    "                # 提取用户名\n",
    "                username_tag = info_tag.select_one('div[data-testid=\"User-Name\"]')\n",
    "                if username_tag:\n",
    "                    username = username_tag.get_text(strip=True)\n",
    "                    # 检查用户名是否包含垃圾词\n",
    "                    if any(garbage in username for garbage in garbages):\n",
    "                        isgarbage = 1\n",
    "                        continue  # 跳过当前帖子，不再提取ID\n",
    "                    username = username_tag.select_one('span').text.strip() if username_tag.select_one('span') else ''\n",
    "                    # print(username)\n",
    "                else:\n",
    "                    # print(\"no username\")\n",
    "                    continue\n",
    "                # # 提取用户名\n",
    "                # username_span = post_tag.find('span', class_='css-1jxf684 r-bcqeeo r-1ttztb7 r-qvutc0 r-poiln3')\n",
    "                # if username_span:\n",
    "                #     username = username_span.get_text()\n",
    "                # else:\n",
    "                #     username = 'Unknown'\n",
    "                # print(username)\n",
    "                isgarbage = 0\n",
    "                \n",
    "\n",
    "                # 提取帖子正文\n",
    "                # post_content = post_tag.select_one('div[data-testid=\"tweetText\"]').text.strip() if post_tag.select_one('div[data-testid=\"tweetText\"]') else ''\n",
    "            # 提取帖子正文，并保持图片和文字顺序\n",
    "                post_content = \"\"\n",
    "                quote_tag = post_tag.find('div', style=\"-webkit-line-clamp: 10; color: rgb(231, 233, 234);\")\n",
    "                if not quote_tag:\n",
    "                    continue\n",
    "                if quote_tag:\n",
    "                    # 遍历所有子元素，按顺序提取文字、图片和链接\n",
    "                    for element in quote_tag.children:\n",
    "                        if element.name == 'span':  # 处理文本\n",
    "                            post_content += element.text.strip() + \" \"\n",
    "                        elif element.name == 'img':  # 处理图片\n",
    "                            alt_text = element.get('alt', '')\n",
    "                            if alt_text:\n",
    "                                post_content += f\"{alt_text} \"\n",
    "                        elif element.name == 'a':  # 处理链接\n",
    "                            href = element.get('href', '')\n",
    "                            if href:\n",
    "                                post_content += f\"{href} \"\n",
    "                                post_content += element.text.strip() + \" \"\n",
    "                # print(post_content)\n",
    "\n",
    "                # 查找包含aria-label的div标签，提取转评赞以及帖子id\n",
    "                cnt_tag = None\n",
    "                if post_time < '2023-01-01':\n",
    "                    # print(post_time)\n",
    "                    aria_label = post_tag.find('div', attrs={'aria-label': re.compile(r'.*Likes.*')}) # likes views\n",
    "                    cnt_tag = post_tag.find_all('button', class_='css-175oi2r r-1777fci r-bt1l66 r-bztko3 r-lrvibr r-1loqt21 r-1ny4l3l')\n",
    "                else:\n",
    "                    aria_label = post_tag.find('div', attrs={'aria-label': re.compile(r'.*views.*')}) # likes views\n",
    "\n",
    "                if cnt_tag and post_time < '2023-01-01':\n",
    "                    # print(\"has cnt_tag\")\n",
    "                    # 从aria-label中提取数据\n",
    "                    label_text = cnt_tag[3]['aria-label']\n",
    "\n",
    "                    # 初始化数据\n",
    "                    replies = reposts = likes = bookmarks = views = 0\n",
    "\n",
    "                    # 使用正则表达式逐项匹配\n",
    "                    # if 'replie' in label_text:\n",
    "                    #     replies = int(re.search(r'(\\d+)\\s+replie', label_text).group(1))\n",
    "                    # if 'repost' in label_text:\n",
    "                    #     reposts = int(re.search(r'(\\d+)\\s+repost', label_text).group(1))\n",
    "                    # if 'like' in label_text:\n",
    "                    #     likes = int(re.search(r'(\\d+)\\s+like', label_text).group(1))\n",
    "                    # if 'bookmark' in label_text:\n",
    "                    #     bookmarks = int(re.search(r'(\\d+)\\s+bookmark', label_text).group(1))\n",
    "                    # 修改代码，处理None的情况\n",
    "                    for t in cnt_tag:\n",
    "                        if 'Like' in t['aria-label']:\n",
    "                            label_text = t['aria-label']\n",
    "                    if 'Like' in label_text:\n",
    "                        match = re.search(r'(\\d+)\\s+Like', label_text)\n",
    "                        likes = int(match.group(1)) if match else 0\n",
    "                    # print(likes)\n",
    "                    for t in cnt_tag:\n",
    "                        if 'Repl' in t['aria-label']:\n",
    "                            label_text = t['aria-label']\n",
    "                    if 'Repl' in label_text:\n",
    "                        match = re.search(r'(\\d+)\\s+Repl', label_text)\n",
    "                        replies = int(match.group(1)) if match else 0\n",
    "                    for t in cnt_tag:\n",
    "                        if 'repost' in t['aria-label']:\n",
    "                            label_text = t['aria-label']\n",
    "                    if 'repost' in label_text:\n",
    "                        match = re.search(r'(\\d+)\\s+repost', label_text)\n",
    "                        reposts = int(match.group(1)) if match else 0\n",
    "                    views = 0\n",
    "\n",
    "                    # 查找帖子ID和用户ID\n",
    "                    post_id_tag = post_tag.find_all('div', class_='css-175oi2r r-18u37iz r-1h0z5md r-13awgt0')\n",
    "                    if post_id_tag:\n",
    "                        # 找到第四个div，提取其中的链接\n",
    "                        post_id_link = post_id_tag[3].find('a', href=True)\n",
    "                        if post_id_link:\n",
    "                            post_id = post_id_link['href'].split('/')[3]  # 从URL中提取帖子ID\n",
    "                            user_id = post_id_link['href'].split('/')[1]\n",
    "                        else:\n",
    "                            post_id = ''\n",
    "                            user_id = ''\n",
    "                    else:\n",
    "                        post_id = ''\n",
    "                        user_id = ''\n",
    "                    # print(post_id)\n",
    "\n",
    "                elif aria_label and post_time >= '2023-01-01':\n",
    "                    # print(\"has aria_label\")\n",
    "                    # 从aria-label中提取数据\n",
    "                    label_text = aria_label['aria-label']\n",
    "\n",
    "                    # 初始化数据\n",
    "                    replies = reposts = likes = bookmarks = views = 0\n",
    "\n",
    "                    # 使用正则表达式逐项匹配\n",
    "                    # if 'replie' in label_text:\n",
    "                    #     replies = int(re.search(r'(\\d+)\\s+replie', label_text).group(1))\n",
    "                    # if 'repost' in label_text:\n",
    "                    #     reposts = int(re.search(r'(\\d+)\\s+repost', label_text).group(1))\n",
    "                    # if 'like' in label_text:\n",
    "                    #     likes = int(re.search(r'(\\d+)\\s+like', label_text).group(1))\n",
    "                    # if 'bookmark' in label_text:\n",
    "                    #     bookmarks = int(re.search(r'(\\d+)\\s+bookmark', label_text).group(1))\n",
    "                    # 修改代码，处理None的情况\n",
    "                    if 'repl' in label_text:\n",
    "                        match = re.search(r'(\\d+)\\s+repl', label_text)\n",
    "                        replies = int(match.group(1)) if match else 0\n",
    "\n",
    "                    if 'repost' in label_text:\n",
    "                        match = re.search(r'(\\d+)\\s+repost', label_text)\n",
    "                        reposts = int(match.group(1)) if match else 0\n",
    "\n",
    "                    if 'like' in label_text:\n",
    "                        match = re.search(r'(\\d+)\\s+like', label_text)\n",
    "                        likes = int(match.group(1)) if match else 0\n",
    "\n",
    "                    if 'bookmark' in label_text:\n",
    "                        match = re.search(r'(\\d+)\\s+bookmark', label_text)\n",
    "                        bookmarks = int(match.group(1)) if match else 0\n",
    "                    if 'view' in label_text:\n",
    "                        # views = int(re.search(r'(\\d+)\\s+view', label_text).group(1))\n",
    "                        # 检查是否匹配到了 \"view\" 和数字\n",
    "                        match = re.search(r'(\\d+)\\s+view', label_text)\n",
    "                        if match:\n",
    "                            views = int(match.group(1))\n",
    "                        else:\n",
    "                            views = 0  # 如果没有匹配到，设置默认值为0\n",
    "\n",
    "                    # 查找帖子ID和用户ID\n",
    "                    post_id_tag = aria_label.find_all('div', class_='css-175oi2r r-18u37iz r-1h0z5md r-13awgt0')\n",
    "                    if post_id_tag:\n",
    "                        # 找到第四个div，提取其中的链接\n",
    "                        post_id_link = post_id_tag[3].find('a', href=True)\n",
    "                        if post_id_link:\n",
    "                            post_id = post_id_link['href'].split('/')[3]  # 从URL中提取帖子ID\n",
    "                            user_id = post_id_link['href'].split('/')[1]\n",
    "                        else:\n",
    "                            post_id = ''\n",
    "                            user_id = ''\n",
    "                    else:\n",
    "                        post_id = ''\n",
    "                        user_id = ''\n",
    "                else:\n",
    "                    replies = reposts = likes = bookmarks = views = 0\n",
    "                    post_id = ''\n",
    "                    user_id = ''\n",
    "                    parent_comment_id = None\n",
    "                    break\n",
    "\n",
    "                # 提取hashtags，确保唯一性，使用集合去重\n",
    "                hashtags = set()\n",
    "                hashtag_tags = quote_tag.select('a[href*=\"/hashtag/\"]')\n",
    "                for tag in hashtag_tags:\n",
    "                    hashtag = tag.get_text(strip=True)\n",
    "                    hashtags.add(hashtag)\n",
    "\n",
    "                # 提取图片和视频的链接\n",
    "                media_urls = set()  # 使用集合来自动去重\n",
    "                img_tags = quote_tag.find_all('img')\n",
    "                for img in img_tags:\n",
    "                    img_url = img.get('src')\n",
    "                    if img_url and 'profile_images' not in img_url and 'emoji' not in img_url:\n",
    "                        media_urls.add(img_url)\n",
    "                media_urls = list(media_urls)\n",
    "\n",
    "                # 提取@了哪些用户\n",
    "                at_usernames = set()  # 使用集合确保唯一性\n",
    "                at_usernames.update(re.findall(r'@(\\w+)', post_content))  # 查找所有 @ 后的用户名\n",
    "\n",
    "                # 提取帖子中的所有链接（使用正则表达式从文本中匹配URL）\n",
    "                urls = set()  # 使用集合去重\n",
    "\n",
    "                # 匹配以 http 或 https 开头的 URL\n",
    "                url_pattern = re.compile(r'https?://[^\\s]+')\n",
    "\n",
    "                # 在帖子文本中查找所有符合条件的URL\n",
    "                urls.update(url_pattern.findall(post_content))\n",
    "\n",
    "                # 查找回复的多个用户\n",
    "                reply_users = []\n",
    "                reply_tag = quote_tag.find('div', class_='css-146c3p1 r-bcqeeo r-1ttztb7 r-qvutc0 r-37j5jr r-a023e6 r-rjixqe r-16dba41') # 查找包含回复信息的 div\n",
    "                if reply_tag:\n",
    "                    # 获取所有回复用户的 a 标签\n",
    "                    reply_user_tags = reply_tag.find_all('a')\n",
    "                    for reply_user_tag in reply_user_tags:\n",
    "                        # 提取用户名并添加到列表\n",
    "                        reply_users.append(reply_user_tag.text.strip())\n",
    "\n",
    "                # 将评论及用户信息存储在字典中\n",
    "                quote_data = {\n",
    "                        \"postUrl\": f\"https://x.com/{user_id}/status/{post_id}\",\n",
    "                        \"mid\": post_id,\n",
    "                        \"text\": post_content,\n",
    "                        \"date\": post_time,\n",
    "                        \"userId\": user_id,\n",
    "                        \"userName\": username,\n",
    "                        \"source_post_id\": main_post_id,  # 主帖ID\n",
    "                        \"is_quote\": True,  # 父级ID\n",
    "                        \"likeNum\": likes,\n",
    "                        \"commentNum\": replies,\n",
    "                        \"repostNum\": reposts,\n",
    "                        \"viewNum\": views,\n",
    "                        \"hashtags\": list(hashtags),\n",
    "                        \"mediaUrls\": media_urls,\n",
    "                        \"atUsernames\": list(at_usernames),  # 新增字段：记录@的用户\n",
    "                        \"urls\": list(urls),  # 新增字段：记录帖中的所有链接\n",
    "                        \"replyUsers\": reply_users  # 新增字段：记录多个回复的用户\n",
    "                }\n",
    "                posts_lists.append(quote_data)\n",
    "                post_index.add(transform_value)\n",
    "\n",
    "                # 保存 posts\n",
    "                with open(posts_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(list(posts_lists), f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        current_post_count = len(posts_lists)\n",
    "        print(f\"已收集到 {current_post_count} 个quote帖\")\n",
    "\n",
    "        if current_post_count >= 3000:  # 如果爬取到的quote条数超过3000，则停止爬取\n",
    "             break\n",
    "\n",
    "        if current_post_count == previous_post_count and not isgarbage: #防止遇到批量垃圾帖子，导致误识别为滑到底了\n",
    "            time.sleep(random.uniform(1, 2))\n",
    "            attempts += 1\n",
    "        else:\n",
    "            attempts = 0\n",
    "            previous_post_count = current_post_count\n",
    "\n",
    "        if attempts == max_attempts:\n",
    "            # 检测到页面错误信息，尝试点击'Retry'按钮...\n",
    "            if len(post_tags) == 0:\n",
    "                print(f\"检测到'Retry'按钮，当前时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')},滚动了{scroll_count}次\")\n",
    "                # 等待人工确认点击\n",
    "                # input(\"按Enter键以继续爬取...\")\n",
    "                while True:  # 无限循环，直到成功点击\n",
    "                    try:\n",
    "                        # 等待6分钟（360秒）\n",
    "                        print(\"等待6分钟...\")\n",
    "                        time.sleep(360)  # 等待6分钟\n",
    "\n",
    "                        # 使用 CSS Selector 或 XPath 查找'Retry'按钮并确保它可点击\n",
    "                        retrybutton = WebDriverWait(driver, 3).until(\n",
    "                            EC.element_to_be_clickable((By.XPATH, \"//button[.//*[contains(text(),'Retry')]]\"))\n",
    "                        )\n",
    "\n",
    "                        # 滚动到按钮所在位置\n",
    "                        driver.execute_script(\"arguments[0].scrollIntoView();\", retrybutton)\n",
    "\n",
    "                        # 强制点击按钮\n",
    "                        driver.execute_script(\"arguments[0].click();\", retrybutton)\n",
    "                        print(f\"点击'Retry'按钮成功，等待5秒后继续爬取，当前时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "                        time.sleep(15)  # 等待15秒后继续爬取\n",
    "                        attempts = 0\n",
    "                        previous_post_count = current_post_count\n",
    "                        break  # 成功点击后跳出循环\n",
    "\n",
    "                    except Exception as e:\n",
    "                        # print(f\"未能找到'Retry'按钮或点击失败，错误信息: {e}\")\n",
    "                        print(\"没找到按钮\")\n",
    "\n",
    "            elif len(post_tags) > 0 and post_tags[-1].select_one('span').text.strip() == \"Something went wrong. Try reloading.\" if post_tags[-1].select_one('span') else '':\n",
    "                print(f\"检测到'Retry'按钮，当前时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')},滚动了{scroll_count}次\")\n",
    "                # 等待人工确认点击\n",
    "                # input(\"按Enter键以继续爬取...\")\n",
    "                while True:  # 无限循环，直到成功点击\n",
    "                    try:\n",
    "                        # 等待6分钟（360秒）\n",
    "                        print(\"等待6分钟...\")\n",
    "                        time.sleep(360)  # 等待6分钟\n",
    "\n",
    "                        # 使用 CSS Selector 或 XPath 查找'Retry'按钮并确保它可点击\n",
    "                        retrybutton = WebDriverWait(driver, 3).until(\n",
    "                            EC.element_to_be_clickable((By.XPATH, \"//button[.//*[contains(text(),'Retry')]]\"))\n",
    "                        )\n",
    "\n",
    "                        # 滚动到按钮所在位置\n",
    "                        driver.execute_script(\"arguments[0].scrollIntoView();\", retrybutton)\n",
    "\n",
    "                        # 强制点击按钮\n",
    "                        driver.execute_script(\"arguments[0].click();\", retrybutton)\n",
    "                        print(f\"点击'Retry'按钮成功，等待5秒后继续爬取，当前时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "                        time.sleep(15)  # 等待15秒后继续爬取\n",
    "                        attempts = 0\n",
    "                        previous_post_count = current_post_count\n",
    "                        break  # 成功点击后跳出循环\n",
    "\n",
    "                    except Exception as e:\n",
    "                        # print(f\"未能找到'Retry'按钮或点击失败，错误信息: {e}\")\n",
    "                        print(\"没找到按钮\")\n",
    "\n",
    "            print(\"多次滚动后没有新数据，等10秒\")\n",
    "            time.sleep(10)\n",
    "\n",
    "        if attempts > max_attempts:  # 如果连续6次没有新数据，停止爬取\n",
    "            print(f\"多次滚动后没有新数据，已尝试 {attempts}\")\n",
    "            break\n",
    "            # user_input = input(\"请输入0停止爬取，1继续爬取: \")  # 获取用户输入\n",
    "            # if user_input == \"0\":\n",
    "            #     print(\"停止爬取。\")\n",
    "            #     break  # 停止爬取\n",
    "            # elif user_input == \"1\":\n",
    "            #     print(\"继续爬取...\")\n",
    "            #     attempts = 0  # 重置尝试次数，继续爬取\n",
    "            # else:\n",
    "            #     print(\"无效输入，请输入0或1。\")\n",
    "\n",
    "        driver.execute_script(f\"window.scrollBy(0, {scroll_step});\")\n",
    "        scroll_count += 1\n",
    "        time.sleep(random.uniform(2, 3))  # 等待加载更多内容\n",
    "\n",
    "# 测试爬取一个帖子的评论\n",
    "# get_post_comments('kohkalah', '1804259338766241930')\n",
    "# get_post_comments('RFKJrHealthSec', '1867589034274340942')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b0c3d3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no quotes\n"
     ]
    }
   ],
   "source": [
    "get_post_quotes(\"OluchiJudith10\", \"1892901957351747733\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928e400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = 'twitter_posts_new'  # 替换为你的文件夹路径\n",
    "posturls_file = '[top rank]posturls_wquotes_genetically modified organism_2014_2025.txt'\n",
    "\n",
    "# 用于存储读取的postUrls\n",
    "post_urls = []\n",
    "\n",
    "# 读取文件中的每一行，去掉换行符并存储到列表中\n",
    "with open(os.path.join(folder_path, posturls_file), 'r', encoding='utf-8') as file:\n",
    "    post_urls = [line.strip() for line in file.readlines()]\n",
    "print(len(post_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e44271",
   "metadata": {},
   "outputs": [],
   "source": [
    "#从ptr_url.txt中读取ptr_url\n",
    "with open(os.path.join(folder_path,'ptr_quote_url.txt'), 'r') as file:\n",
    "    file_content = file.readlines()\n",
    "    ptr_quote_url = file_content[0] if len(file_content) else ''\n",
    "print(ptr_quote_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e5c487",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesleep_cnt = 0\n",
    "after_ptr_url = False\n",
    "for post_url in post_urls:\n",
    "    if post_url == ptr_quote_url or ptr_quote_url == '':\n",
    "        print(\"到达指针位置\")\n",
    "        after_ptr_url = True\n",
    "    if after_ptr_url:\n",
    "        user_id = post_url.split('/')[3]\n",
    "        post_id = post_url.split('/')[5]\n",
    "        print(f\"正在爬取帖子quote：{user_id}/{post_id}\")\n",
    "        ptr_quote_url = post_url\n",
    "        # 存储ptr_quote_url到文件\n",
    "        with open(os.path.join(folder_path, 'ptr_quote_url.txt'), 'w', encoding='utf-8') as file:\n",
    "            file.write(ptr_quote_url)\n",
    "        get_post_quotes(user_id, post_id)\n",
    "        timesleep_cnt += 1\n",
    "        if timesleep_cnt % 50 == 0:\n",
    "            print(f\"已爬取{timesleep_cnt}个主帖的quote，休息2分钟...\")\n",
    "            time.sleep(120)\n",
    "        else:\n",
    "            time.sleep(3)\n",
    "    else:\n",
    "        print(f\"{post_url}已被采集过quote\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1057eb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afc75a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标题: X\n"
     ]
    }
   ],
   "source": [
    "#先运行命令再运行此代码块 & \"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\" --remote-debugging-port=9530 --user-data-dir=\"D:\\selenium\\AutomationProfile\"\n",
    "chromedriver_path = \"C:/Program Files/Google/Chrome/Application/chromedriver.exe\"\n",
    "\n",
    "option = Options()\n",
    "option.add_experimental_option(\"debuggerAddress\", \"127.0.0.1:9530\")\n",
    "\n",
    "# 创建 ChromeDriver 服务\n",
    "service = Service(chromedriver_path)\n",
    "\n",
    "# 初始化 WebDriver\n",
    "driver = webdriver.Chrome(service=service, options=option)\n",
    "\n",
    "# 去除 WebDriver 痕迹\n",
    "driver.execute_cdp_cmd(\"Page.addScriptToEvaluateOnNewDocument\", {\n",
    "    \"source\": \"\"\"\n",
    "    Object.defineProperty(navigator, 'webdriver', {\n",
    "      get: () => undefined\n",
    "    });\n",
    "    \"\"\"\n",
    "})\n",
    "\n",
    "# 设置隐式等待\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# 测试访问页面\n",
    "driver.get(\"https://www.x.com\")\n",
    "print(\"标题:\", driver.title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb2cdb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#修正数字\n",
    "import re\n",
    "def parse_number(value):\n",
    "    try:\n",
    "        # 使用正则表达式提取数字和小数部分\n",
    "        match = re.search(r'[\\d.]+', value)\n",
    "        if match:\n",
    "            number = match.group()\n",
    "            if 'K' in value:\n",
    "                return int(float(number) * 1000)\n",
    "            elif 'M' in value:\n",
    "                return int(float(number) * 1000000)\n",
    "            elif 'B' in value:\n",
    "                return int(float(number) * 100000000)\n",
    "            else:\n",
    "                return int(float(number))  # 支持小数部分\n",
    "        else:\n",
    "            raise ValueError(\"未找到数值部分\")\n",
    "    except (ValueError, TypeError) as e:\n",
    "        print(f\"无法解析数值：{value}，错误：{e}\")\n",
    "        return value  # 返回原始值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5db170fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取个人主页信息\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "#单个用户主页信息采集\n",
    "def get_user_profiles(profile_url, user_id):\n",
    "    # 打开指定的个人主页链接\n",
    "    driver.get(profile_url)\n",
    "    user_info = {}\n",
    "\n",
    "    try:\n",
    "        # 等待导航栏标签加载完成\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//nav[@aria-label='Profile timelines']\"))\n",
    "        )\n",
    "    except Exception as e:\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        #检查是否为异常账户\n",
    "        if soup.find('div', {'data-testid': 'emptyState'}):\n",
    "            return 1  #识别为异常账户\n",
    "        return 0\n",
    "\n",
    "    time.sleep(random.uniform(3, 4.5))\n",
    "\n",
    "    # 获取页面 HTML\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # 获取用户名\n",
    "    username_tag = soup.find('div', {'data-testid': 'UserName'})\n",
    "    if username_tag:\n",
    "        user_info['userName'] = username_tag.find('span').text.strip()\n",
    "\n",
    "    # 获取id\n",
    "    user_info['userId'] = user_id\n",
    "\n",
    "    # 获取intro\n",
    "    nickname_tag = soup.find('div', {'data-testid': 'UserDescription'})\n",
    "    if nickname_tag:\n",
    "        user_info['intro'] = nickname_tag.text.strip()\n",
    "    else:\n",
    "        user_info['intro'] =  \"\"\n",
    "\n",
    "    #获取Profession职业信息\n",
    "    profession_tag = soup.find('span', {'data-testid': 'UserProfessionalCategory'})\n",
    "    if profession_tag:\n",
    "        user_info['profession'] = profession_tag.text.strip()\n",
    "\n",
    "    #获取birthdate\n",
    "    birthdate_tag = soup.find('span', {'data-testid': 'UserBirthdate'})\n",
    "    if birthdate_tag:\n",
    "        user_info['birthdate'] = birthdate_tag.text.strip()\n",
    "\n",
    "    # 获取用户网址\n",
    "    user_url_tag = soup.find('a', {'data-testid': 'UserUrl'})\n",
    "    if user_url_tag:\n",
    "        user_info['userUrl'] = user_url_tag['href'].strip()\n",
    "\n",
    "    # 获取关注数\n",
    "    following_tag = soup.find('a', {'href': lambda x: x and '/following' in x})\n",
    "    if following_tag:\n",
    "        follow_count_text = following_tag.find('span', recursive=False).text.strip() if following_tag.find('span',\n",
    "                                                                                                           recursive=False) else following_tag.text.strip()\n",
    "        user_info['following'] = parse_number(follow_count_text)\n",
    "\n",
    "    # 获取粉丝数\n",
    "    followers_tag = soup.find('a', {'href': lambda x: x and '/verified_followers' in x})\n",
    "    if followers_tag:\n",
    "        followers_count_text = followers_tag.find('span', recursive=False).text.strip() if followers_tag.find('span',\n",
    "                                                                                                              recursive=False) else followers_tag.text.strip()\n",
    "        user_info['followers'] = parse_number(followers_count_text)\n",
    "\n",
    "    # 获取订阅数\n",
    "    subscriptions_tag = soup.find('a', {'href': lambda x: x and '/subscriptions' in x})\n",
    "    if subscriptions_tag:\n",
    "        subscriptions_count_text = subscriptions_tag.find('span', recursive=False).text.strip() if subscriptions_tag.find(\n",
    "            'span', recursive=False) else subscriptions_tag.text.strip()\n",
    "        user_info['subscriptions'] = parse_number(subscriptions_count_text)\n",
    "\n",
    "    # 获取地点\n",
    "    location_tag = soup.find('span', {'data-testid': 'UserLocation'})\n",
    "    if location_tag:\n",
    "        user_info['location'] = location_tag.text.strip()\n",
    "\n",
    "    return user_info\n",
    "\n",
    "#遍历采集，等待机制\n",
    "def start_get_user_profiles(user_ids):\n",
    "\n",
    "    print(f\"需要爬取信息的用户ID总数: {len(user_ids)}个\")\n",
    "\n",
    "    # 创建存储用户资料信息的目录\n",
    "    base_dir = f\"./GMO/users\"\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d-%H-%M\") \n",
    "    # 确定已有文件的初始状态\n",
    "    current_file_path = os.path.join(base_dir, f\"user_profiles_{current_date}.json\")\n",
    "    user_profiles = []\n",
    "\n",
    "    if not os.path.exists(current_file_path):\n",
    "        with open(current_file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump([], f)  # 写入空数组\n",
    "    else:\n",
    "        with open(current_file_path, 'r', encoding='utf-8') as f:\n",
    "            user_profiles = json.load(f)\n",
    "\n",
    "    current_index = len(user_profiles)\n",
    "\n",
    "    # 记录上次保存的数量\n",
    "    last_saved_count = len(user_profiles)\n",
    "\n",
    "    # 从当前索引位置开始爬取用户信息\n",
    "    user_ids = user_ids[current_index:]\n",
    "    print(f\"开始爬取用户主页信息，当前已有数据量: {current_index}\")\n",
    "\n",
    "    # 获取用户信息\n",
    "    with tqdm(total=len(user_ids), desc=\"获取用户信息进度\", unit=\"用户\") as pbar:\n",
    "        for idx, user_id in enumerate(user_ids):\n",
    "            profile_url = f'https://x.com/{user_id}'\n",
    "            retrys = 6\n",
    "            for retry in range(retrys):\n",
    "                user_info = get_user_profiles(profile_url, user_id)\n",
    "                if user_info:\n",
    "                    if user_info == 1:  # 识别出异常账户，跳过\n",
    "                        break\n",
    "                    user_profiles.append(user_info)\n",
    "                    break\n",
    "                else:\n",
    "                    if retry == 0:\n",
    "                        print(f\"第{retry + 1}次尝试，未能获取到用户 {user_id} 的信息, 休息20秒\")\n",
    "                        time.sleep(20)\n",
    "                    elif retry == 5:\n",
    "                        print(f\"获取 {user_id} 的信息失败，跳过\")\n",
    "                    else:\n",
    "                        print(f\"第{retry + 1}次尝试，未能获取到用户 {user_id} 的信息, 休息六分钟，当前时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "                        time.sleep(365)\n",
    "\n",
    "            # 每次到达 20 个用户信息时，保存一次\n",
    "            if len(user_profiles) >= last_saved_count + 20:\n",
    "                with open(current_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(user_profiles, f, ensure_ascii=False, indent=4)\n",
    "                last_saved_count = len(user_profiles)\n",
    "                print(f\"已保存 {len(user_profiles)} 个用户主页信息到 {current_file_path}\")\n",
    "\n",
    "            time.sleep(random.uniform(1, 1.5))  # 随机休息\n",
    "            pbar.update(1)\n",
    "\n",
    "    # 保存剩余的用户信息（不足 20 个的部分）\n",
    "    if len(user_profiles) > last_saved_count:\n",
    "        with open(current_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(user_profiles, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"已保存剩余的 {len(user_profiles) - last_saved_count} 个用户主页信息到 {current_file_path}\")\n",
    "\n",
    "    print(\"所有用户主页信息已保存完毕\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b1ae51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25233\n"
     ]
    }
   ],
   "source": [
    "# 将GMO/user_id.txt中的内容load到user_ids\n",
    "user_ids = []\n",
    "with open('GMO/[top rank]userids_genetically modified organism_2014_2025.txt', 'r', encoding='utf-8') as f:\n",
    "    user_ids = [line.strip() for line in f.readlines()]\n",
    "print(len(user_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c56312c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25037\n"
     ]
    }
   ],
   "source": [
    "# 把ptr_user.txt中的内容加载到ptr_user中\n",
    "with open('GMO/users/ptr_user.txt', 'r') as file:\n",
    "    file_content = file.readlines()\n",
    "    ptr_user = file_content[0] if len(file_content) else ''\n",
    "i = 0\n",
    "for user_id in user_ids:\n",
    "    if user_id == ptr_user:\n",
    "        break\n",
    "    i = i + 1\n",
    "user_ids = user_ids[i:]\n",
    "print(len(user_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a018e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "需要爬取信息的用户ID总数: 25233个\n",
      "开始爬取用户主页信息，当前已有数据量: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "获取用户信息进度:   0%|          | 20/25233 [02:16<42:42:35,  6.10s/用户]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存 20 个用户主页信息到 ./GMO/users\\user_profiles.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "获取用户信息进度:   0%|          | 40/25233 [04:21<48:16:14,  6.90s/用户]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存 40 个用户主页信息到 ./GMO/users\\user_profiles.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "获取用户信息进度:   0%|          | 60/25233 [06:22<42:40:57,  6.10s/用户]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存 60 个用户主页信息到 ./GMO/users\\user_profiles.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "获取用户信息进度:   0%|          | 80/25233 [08:27<51:14:14,  7.33s/用户]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存 80 个用户主页信息到 ./GMO/users\\user_profiles.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "获取用户信息进度:   0%|          | 100/25233 [10:49<48:16:23,  6.91s/用户]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存 100 个用户主页信息到 ./GMO/users\\user_profiles.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "获取用户信息进度:   0%|          | 120/25233 [13:13<48:06:20,  6.90s/用户]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存 120 个用户主页信息到 ./GMO/users\\user_profiles.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "获取用户信息进度:   1%|          | 140/25233 [15:37<47:39:01,  6.84s/用户]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存 140 个用户主页信息到 ./GMO/users\\user_profiles.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "获取用户信息进度:   1%|          | 160/25233 [17:59<48:18:25,  6.94s/用户]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存 160 个用户主页信息到 ./GMO/users\\user_profiles.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "获取用户信息进度:   1%|          | 180/25233 [20:28<53:34:40,  7.70s/用户]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存 180 个用户主页信息到 ./GMO/users\\user_profiles.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "获取用户信息进度:   1%|          | 196/25233 [22:15<45:30:50,  6.54s/用户]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1次尝试，未能获取到用户 2w1t2w1tter 的信息, 休息20秒\n",
      "第2次尝试，未能获取到用户 2w1t2w1tter 的信息, 休息六分钟，当前时间: 2025-02-25 14:14:53\n"
     ]
    }
   ],
   "source": [
    "start_get_user_profiles(user_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datacrawl_backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
